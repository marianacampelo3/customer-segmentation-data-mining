{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_DBXuj9m7JR"
      },
      "source": [
        "# **DATA MINING PROJECT I**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgrK3EO7OCdl"
      },
      "source": [
        "#Importing and loading the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing\n",
        "\n"
      ],
      "metadata": {
        "id": "VGUPocP6YDgJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NCwGevbLH73"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sqlite3\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "from itertools import product\n",
        "from math import ceil\n",
        "\n",
        "# Data manipulation and math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import math\n",
        "\n",
        "# Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import graphviz\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "\n",
        "# Preprocessing and dimensionality reduction\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Clustering and evaluation metrics\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN, estimate_bandwidth\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples, pairwise_distances\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "\n",
        "# Visualisation settings\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set()\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import pydotplus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu1HAYR2OKcv"
      },
      "source": [
        "## Loading Data\n",
        "\n",
        "We then give access to google Colab of our google drive to be able to import the ```customer_info.csv``` to our project. We then import the file and create a pandas dataframe named - ```df```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5SFPG6v7r41"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"customer_info.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U42-rpUvolCI"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_sXaq79P-Ph"
      },
      "source": [
        "## Metadata  \n",
        "\n",
        "- *customer_id* - Identifier of the customer  \n",
        "- *customer_name* - Name of the customer (contains degree level)  \n",
        "- *customer_gender* - Gender of the customer  \n",
        "- *customer_birth_date* - Birth date of the customer  \n",
        "- *kids_home* - Number of kids at home  \n",
        "- *teen_home* - Number of teens at home  \n",
        "- *number_complaints* - Number of complaints formally done by the customer  \n",
        "- *distinct_stores_visited* - Number of distinct stores visited by the customer  \n",
        "- *lifetime_spend_groceries* - Total value spent by the customer on groceries  \n",
        "- *lifetime_spend_electronics* - Total value spent by the customer on electronics  \n",
        "- *typical_hour* - Typical hour when the customer visits the store  \n",
        "- *lifetime_spend_vegetables* - Total value spent by the customer on vegetables  \n",
        "- *lifetime_spend_nonalcohol_drinks* - Total value spent by the customer on non-alcoholic drinks  \n",
        "- *lifetime_spend_alcohol_drinks* - Total value spent by the customer on alcoholic drinks  \n",
        "- *lifetime_spend_meat* - Total value spent by the customer on meat  \n",
        "- *lifetime_spend_fish* - Total value spent by the customer on fish  \n",
        "- *lifetime_spend_hygiene* - Total value spent by the customer on hygiene  \n",
        "- *lifetime_spend_videogames* - Total value spent by the customer on video games  \n",
        "- *lifetime_spend_petfood* - Total value spent by the customer on pet food  \n",
        "- *lifetime_total_distinct_products* - Number of distinct products bought by the customer (lifetime)  \n",
        "- *percentage_of_products_bought_promotion* - Percentage of products that were bought with some promotion  \n",
        "- *year_first_transaction* - Year of the first transaction of the customer  \n",
        "- *loyalty_card_number* - Number of the customer loyalty card  \n",
        "- *location_latitude* - Approximate location (<1km range) of the customer's home (Latitude)  \n",
        "- *location_longitude* - Approximate location (<1km range) of the customer's home (Longitude)  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljJpu_1eQGXv"
      },
      "source": [
        "# 2. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of the project emcompasses the initial Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "3L1-jQK9Rmeg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcCifDEhPGRi"
      },
      "outputs": [],
      "source": [
        "# Visualising the first 10 rows of the df\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Was checked the different column types in the dataframe"
      ],
      "metadata": {
        "id": "_9UgMibhR1Ek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq73NEvLQLc0"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cloumns were rename on our pandas dataframe for clarity and simplification"
      ],
      "metadata": {
        "id": "z7pZ3I7VTa9u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_AoiPjrfxpY"
      },
      "outputs": [],
      "source": [
        "df = df.rename(columns={'customer_id': 'id',\n",
        "    'customer_name': 'name',\n",
        "    'customer_gender': 'gender',\n",
        "    'customer_birthdate': 'birthdate',\n",
        "    'kids_home': 'n_kids',\n",
        "    'teens_home': 'n_teens',\n",
        "    'number_complaints': 'n_complaints',\n",
        "    'distinct_stores_visited': 'n_stores',\n",
        "    'lifetime_spend_groceries': 'spend_groceries',\n",
        "    'lifetime_spend_electronics': 'spend_electronics',\n",
        "    'lifetime_spend_vegetables': 'spend_vegetables',\n",
        "    'lifetime_spend_nonalcohol_drinks': 'spend_nonalcohol',\n",
        "    'lifetime_spend_alcohol_drinks': 'spend_alcohol',\n",
        "    'lifetime_spend_meat': 'spend_meat',\n",
        "    'lifetime_spend_fish': 'spend_fish',\n",
        "    'lifetime_spend_hygiene': 'spend_hygiene',\n",
        "    'lifetime_spend_videogames': 'spend_videogames',\n",
        "    'lifetime_spend_petfood': 'spend_petfood',\n",
        "    'lifetime_total_distinct_products': 'n_distinct_products',\n",
        "    'percentage_of_products_bought_promotion': 'pct_promo',\n",
        "    'year_first_transaction': 'year_first_transaction',\n",
        "    'loyalty_card_number': 'loyalty_card'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2flhFkuh5BlT"
      },
      "source": [
        "## Missing Values\n",
        "\n",
        "After examining the project dataset, our team has conducted an analysis of the missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OeNXIm8QW4B"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv4SBZo3c97I"
      },
      "outputs": [],
      "source": [
        "# Checking the percentage of missing values in each column\n",
        "\n",
        "missing_counts = df.isna().sum()\n",
        "total_rows = len(df)\n",
        "\n",
        "missing_percent = (missing_counts / total_rows) * 100\n",
        "missing_percent = missing_percent.round(2)\n",
        "\n",
        "missing_percent_str = missing_percent.astype(str) + \" %\"\n",
        "\n",
        "missing_percent_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkTpyY5i5bsk"
      },
      "outputs": [],
      "source": [
        "df.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is largely complete, with most features showing missing values below 1%. The only significant outlier is loyalty_card, which lacks data for 18.51% of the entries. To resolve this, k-Nearest Neighbors (kNN) Imputation is utilized. This method fills the gaps by identifying patterns and similarities among the existing data points, ensuring a complete dataset for further analysis."
      ],
      "metadata": {
        "id": "K7o4K7GDOraE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3gbDyUw5h4i"
      },
      "source": [
        "## Duplicated Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AICl-RYhQc3p"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDJUJV4TfnU5"
      },
      "outputs": [],
      "source": [
        "# Checking for duplicated values in the column - id\n",
        "df[\"id\"].duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqtpftfOh3Cj"
      },
      "outputs": [],
      "source": [
        "# Checking for duplicated values in the column - loyalty_card\n",
        "df[\"loyalty_card\"].duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The column ```loyalty_card``` has 4,058 duplicated values. It is therefore important to assess whether these duplicated loyalty card numbers correspond to the same client appearing multiple times or whether a single loyalty card number is being associated with different clients.\n",
        "\n",
        "To investigate this, all duplicated loyalty card records were isolated and grouped by loyalty_card. For each loyalty card ID, the number of unique values in the variables name, birthdate, and latitude was computed. These fields act as client identifiers and allow us to detect potential inconsistencies.\n",
        "\n",
        "If a loyalty card ID shows only one unique value across these attributes, it suggests repeated records of the same client.\n",
        "\n",
        "Conversely, loyalty card IDs with multiple unique values in any of these fields indicate that the same loyalty card number may be linked to different clients"
      ],
      "metadata": {
        "id": "21Q50IOuUpWF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du-TkbKlwCUe"
      },
      "outputs": [],
      "source": [
        "#Selecting all the rows which have a duplicated loyaty card\n",
        "dupes = df[df.duplicated('loyalty_card', keep=False)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWP4vzoOv0PA"
      },
      "outputs": [],
      "source": [
        "# Group by loyalty_card ID and count unique values in 'name', 'birthdate', 'latitude' to identify\n",
        "#data inconsistencies\n",
        "\n",
        "check = dupes.groupby('loyalty_card')[['name', 'birthdate', 'latitude']].nunique()\n",
        "print(check)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking an example from the above output, the customer with the loyalty card 900084.0 is associated with two different customers. These customers have different names, different birthdates, and different addresses (as indicated by different latitude values).\n",
        "\n",
        "This is not a simple case of the same customer being entered twice by mistake. This is a case where the same loyalty card number is being registered to different clients."
      ],
      "metadata": {
        "id": "9YqtBrX_cPA2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HX0hJ1Pxw-w"
      },
      "outputs": [],
      "source": [
        "# Filter records where all comparison checks are equal to 1\n",
        "# This means the records match on name, birthdate, and latitude\n",
        "possible_duplicates = check[\n",
        "    (check['name'] == 1) &\n",
        "    (check['birthdate'] == 1) &\n",
        "    (check['latitude'] == 1)\n",
        "]\n",
        "\n",
        "# Step 4: Check how many potential duplicates were found\n",
        "print(len(possible_duplicates))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result of 0 confirms that there are no \"clean\" duplicates in the dataset. This means that whenever a loyalty card ID appears multiple times, it never refers to the same person.\n",
        "\n",
        "With the results from above, and to make the dataset easier for machine learning models to process we have converterd the column ```loyalty_card``` data into a simple numerical indicator. This new column called ```has_card```  assigns a 1 if the customer has a loyalty card and a 0 if the field is empty. We have implied this from Business Rules\n",
        "\n",
        "We then remove the original card ID column and ensure the new variable is stored as a float to maintain consistency with other numerical data. This transformation simplifies the data from a complex ID number into a straightforward \"Yes/No\" feature that represents customer loyalty."
      ],
      "metadata": {
        "id": "RF7pMOe7gP5a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxJf7DwmmhF3"
      },
      "outputs": [],
      "source": [
        "df[\"has_card\"] = df[\"loyalty_card\"].notna().astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6Qbh-kECuNO"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['loyalty_card'], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdAqmKXpJVbs"
      },
      "outputs": [],
      "source": [
        "df['has_card'] = df['has_card'].astype(float)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descriptive Statistics"
      ],
      "metadata": {
        "id": "bdYx6Se9mE33"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mR1MK9sTWyZ"
      },
      "outputs": [],
      "source": [
        "# Descriptive statistics - Numeric features only\n",
        "df.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3yeVgTXTPwy"
      },
      "outputs": [],
      "source": [
        "# Descriptive statistics - All features\n",
        "df.describe(include=\"all\").T"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From looking to the descriptive statitstics from above we can see 2 clear data quality issues.\n",
        "\n",
        "First, the ```pct_promotion``` column contains negative values, which is technically impossible as this variable should strictly range between 0 and 1.\n",
        "\n",
        "Second, the ```year_first_transaction``` columns includes entries for the year 2026. This is logically inconsistent because the dataset was extracted in 2025.\n"
      ],
      "metadata": {
        "id": "jdIARqEKqm1O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9xQPHpq7QYQ"
      },
      "source": [
        "## Numerical/Categorical Split\n",
        "\n",
        "We have split the columns of our DataFrame into two groups - categorical and numerical - to simplify data handling based on their data type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj10hQ2k7e5M"
      },
      "outputs": [],
      "source": [
        "categorical = [\"gender\"]\n",
        "\n",
        "numerical = [\"n_kids\", \"n_teens\", \"n_complaints\", \"n_stores\", \"spend_groceries\", \"spend_electronics\", \"typical_hour\", \"spend_vegetables\", \"spend_nonalcohol\",\"spend_alcohol\", \"spend_meat\",\n",
        "    \"spend_fish\", \"spend_hygiene\",\"spend_videogames\", \"spend_petfood\",\"n_distinct_products\", \"pct_promo\", \"year_first_transaction\", \"latitude\",\"longitude\", \"has_card\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jFKKIRpBsE8"
      },
      "source": [
        "## Incoherences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section of the notebook we start by looking at the incoherence of having some rows where the year of ```year_first_transaction``` is 2026 which is not possible since the data was collected in 2025.\n",
        "\n",
        "This data error was identified in point 2.3 when we analysed the Descriptive statistics\n"
      ],
      "metadata": {
        "id": "4lqil5XYsYbO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RTVqXqle0j-"
      },
      "outputs": [],
      "source": [
        "df[\"year_first_transaction\"].value_counts().sort_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JiCjKFmf2Hw"
      },
      "outputs": [],
      "source": [
        "# Checking the percentage of rows that have the year of first transaction as 2026\n",
        "(df[\"year_first_transaction\"] == 2026).mean() * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-1rhJ1Klly7"
      },
      "outputs": [],
      "source": [
        "# As it was a very small percentage, we have deleted these rows\n",
        "df = df[df[\"year_first_transaction\"] != 2026]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After assessing that only 0.87% of the rows have a ```year_first_transaction``` of 2026 we have decided to remove them from our dataframe as they account for a very small percentage of the total rows."
      ],
      "metadata": {
        "id": "DbNpE3Lp_D39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After checking the incoherences on the ```year_first_transaction``` column we looked into the incoherences of the ```pct_promo```.\n",
        "\n",
        "We have identified that there are values of the pct_promo which are below 0 and above 1. From a Business perspective this cannot happen as this value represents the percentage of products, each client has bought with a discount"
      ],
      "metadata": {
        "id": "bxnZB6Y_Akn0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uch8t9Toicqf"
      },
      "outputs": [],
      "source": [
        "mask_invalid = (df[\"pct_promo\"] < 0) | (df[\"pct_promo\"] > 1)\n",
        "\n",
        "pct_invalid = mask_invalid.mean() * 100\n",
        "n_invalid = mask_invalid.sum()\n",
        "\n",
        "pct_invalid, n_invalid\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we assessed the percentage of entries that have the ```pct_promo```invalid.\n",
        "\n",
        "3.7% of the entries have an invalid value for the ```pct_promo``` which accounts for 622 rows in our dataframe\n",
        "\n"
      ],
      "metadata": {
        "id": "0QPLPp4RBk0K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS2cWF7ricF2"
      },
      "outputs": [],
      "source": [
        "df.loc[mask_invalid, \"pct_promo\"].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we checked how many of these 622 errors were \"close\" to the boundaries of between -10% and 115%."
      ],
      "metadata": {
        "id": "ciICgK83CS1N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blftxPztkYO2"
      },
      "outputs": [],
      "source": [
        "(df.loc[mask_invalid, \"pct_promo\"]\n",
        " .between(-0.10, 1.15)\n",
        " .mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number above stated that 76% of the errors are very small, just slightly below 0 or slightly above 1 (-10% and 115% interval).\n",
        "\n",
        "Since most of the errors were very close to the boundaries instead of deleting the rows all the values that were below 0 in ```pct_promo``` we have set it to 0. All the numbers that were above 1, we have set it to 1. This step was achieved using the code line below\n"
      ],
      "metadata": {
        "id": "LP1x_5BnCe16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRvWiYPmO1hR"
      },
      "outputs": [],
      "source": [
        "df[\"pct_promo\"] = df[\"pct_promo\"].clip(0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j3RMu0M9rvV"
      },
      "source": [
        "## Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7oCeO4b9uwn"
      },
      "outputs": [],
      "source": [
        "# Setting the styles for the graphs\n",
        "sns.set(style=\"darkgrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSmYmdM-9ta8"
      },
      "source": [
        "### Histograms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid overcrowding the graphics and maintain clarity, we have divided the variables into two groups: spend-related variables and other variables."
      ],
      "metadata": {
        "id": "N-K9HnBKEz1i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahigIqoT_AYr"
      },
      "outputs": [],
      "source": [
        "spend =[\"spend_groceries\", \"spend_electronics\", \"spend_vegetables\", \"spend_nonalcohol\",\"spend_alcohol\", \"spend_meat\",\n",
        "    \"spend_fish\", \"spend_hygiene\",\"spend_videogames\", \"spend_petfood\"]\n",
        "others = [\"n_kids\", \"n_teens\", \"n_complaints\", \"n_stores\", \"n_distinct_products\", \"pct_promo\", \"year_first_transaction\", \"latitude\",\"longitude\", \"typical_hour\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvto8zul91tj"
      },
      "outputs": [],
      "source": [
        "# Code to produce the histograms visualisation for the ´spend´ variables\n",
        "cols = 3\n",
        "rows = math.ceil(len(spend) / cols)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(spend):\n",
        "    sns.histplot(x=df[feature], ax=axes[i], kde=True, bins=30)\n",
        "    axes[i].set_title(f\"Histogram of {feature}\")\n",
        "    axes[i].tick_params(axis=\"x\", rotation=0)\n",
        "\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple analysis from the graphs above allows us to identify that most of the ```spend``` variables histogram plots are right skewed with the exception of the histogram for ```spend_pet_food```which has a normal distribution"
      ],
      "metadata": {
        "id": "lPN9LvErFyN6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdQeI7Az_maI"
      },
      "outputs": [],
      "source": [
        "# Code to produce the histograms visualisation for the ´others´ variables\n",
        "\n",
        "cols = 3\n",
        "rows = math.ceil(len(others) / cols)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(others):\n",
        "    sns.histplot(x=df[feature], ax=axes[i], kde=True, bins=30)\n",
        "    axes[i].set_title(f\"Histogram of {feature}\")\n",
        "    axes[i].tick_params(axis=\"x\", rotation=0)\n",
        "\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided histograms and density plots visualize the distribution of various numerical features, highlighting skewed data in variables such as ```n_kids``` and ```n_stores```, alongside more normally distributed patterns in ```pct_promo``` and geographical coordinates."
      ],
      "metadata": {
        "id": "7LCQMv9sSJHy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HayhqYupAC46"
      },
      "source": [
        "### Boxplots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmNHC9zpAE_N"
      },
      "outputs": [],
      "source": [
        "# Code to create the box plots for the numerical variables\n",
        "\n",
        "cols = 3\n",
        "rows = math.ceil(len(numerical) / cols)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(numerical):\n",
        "    sns.boxplot(x=df[feature], ax=axes[i])\n",
        "    axes[i].set_title(f\"Boxplot of {feature}\")\n",
        "\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The numerical variables are analyzed through boxplots to identify distribution patterns and outliers. Significant right-skewness and numerous outliers are identified in all spending categories, such as `spend_groceries` and `spend_meat`, as well as in behavioral counts like `n_kids` and `n_stores`. Conversely, a more balanced and symmetrical spread is observed for `latitude`, `longitude`, and `pct_promo`. Additionally, some isolated outliers are noted in the lower range of `year_first_transaction`, representing earlier customer acquisitions."
      ],
      "metadata": {
        "id": "oGukdZIkSyqM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odK7ox-MAW1I"
      },
      "source": [
        "### Countplots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGIrsWgoAZUv"
      },
      "outputs": [],
      "source": [
        "# Countplot for the only categorical category - gender\n",
        "\n",
        "cols = 3\n",
        "rows = math.ceil(len(categorical) / cols)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(15, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(categorical):\n",
        "    sns.countplot(x=df[feature].dropna(), ax=axes[i], legend=False)\n",
        "    axes[i].set_title(f\"Countplot of {feature}\")\n",
        "    axes[i].tick_params(axis=\"x\", rotation=45)\n",
        "\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The categorical distribution of the ```gender``` feature is illustrated through a countplot, which indicates a nearly balanced representation between the two classes."
      ],
      "metadata": {
        "id": "dqMmdicxTykC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Pre-processing"
      ],
      "metadata": {
        "id": "lGdS-yM7Nei3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYtSZXjYCdtr"
      },
      "source": [
        "## Removing Outliers and Log Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen in the chapter 2.6.1 there are multiple histograms that are right-skewed."
      ],
      "metadata": {
        "id": "-y-eQD_7N_L9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxT2TcSkpHHe"
      },
      "outputs": [],
      "source": [
        "df[spend].skew().sort_values(ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrU_qw8WpkoC"
      },
      "outputs": [],
      "source": [
        "spend_rsk =[\"spend_groceries\", \"spend_electronics\", \"spend_vegetables\", \"spend_nonalcohol\",\"spend_alcohol\", \"spend_meat\", \"spend_fish\", \"spend_hygiene\",\"spend_videogames\" ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-otm8fglLzS"
      },
      "outputs": [],
      "source": [
        "for col in spend_rsk:\n",
        "    df[col + \"_log\"] = np.log1p(df[col])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "je17mzhmnIn1"
      },
      "outputs": [],
      "source": [
        "# Original\n",
        "df[spend_rsk].skew()\n",
        "\n",
        "# transformed log\n",
        "df[[c + \"_log\" for c in spend_rsk]].skew()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxkttI5zEe4F"
      },
      "outputs": [],
      "source": [
        "numerical.append(\"spend_groceries_log\")\n",
        "numerical.append(\"spend_electronics_log\")\n",
        "numerical.append(\"spend_vegetables_log\")\n",
        "numerical.append(\"spend_nonalcohol_log\")\n",
        "numerical.append(\"spend_alcohol_log\")\n",
        "numerical.append(\"spend_meat_log\")\n",
        "numerical.append(\"spend_fish_log\")\n",
        "numerical.append(\"spend_hygiene_log\")\n",
        "numerical.append(\"spend_videogames_log\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine the most effective representation for skewed variables, a comparative analysis was performed between original and log-transformed features using **Z-Score** (|z| > 3) and **Interquartile Range (IQR)** methods. The primary objective was to identify which version minimized the presence of statistical outliers to ensure a more robust dataset.\n"
      ],
      "metadata": {
        "id": "T0rxxozIXsRo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OyEdPx3r0Py"
      },
      "outputs": [],
      "source": [
        "z_scores = np.abs(stats.zscore(df[numerical], nan_policy=\"omit\"))\n",
        "\n",
        "z_outliers = {}\n",
        "\n",
        "for i, col in enumerate(numerical):\n",
        "    n_outliers = (z_scores[:, i] >= 3).sum()\n",
        "    z_outliers[col] = n_outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyEK5V6tr35H"
      },
      "outputs": [],
      "source": [
        "z_outliers_df = (\n",
        "    pd.DataFrame.from_dict(z_outliers, orient=\"index\", columns=[\"n_outliers\"])\n",
        "    .assign(pct_outliers=lambda x: x[\"n_outliers\"] / len(df) * 100)\n",
        "    .sort_index()\n",
        ")\n",
        "\n",
        "z_outliers_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQmQUt_ktgrv"
      },
      "outputs": [],
      "source": [
        "outliers_iqr = {}\n",
        "for col in numerical:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_lim = Q1 - 1.5 * IQR\n",
        "    upper_lim = Q3 + 1.5 * IQR\n",
        "\n",
        "    n_outliers = ((df[col] < lower_lim) | (df[col] > upper_lim)).sum()\n",
        "\n",
        "    outliers_iqr[col] = n_outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GLc5y4R0bP9"
      },
      "outputs": [],
      "source": [
        "outliers_iqr_df = (\n",
        "    pd.DataFrame.from_dict(outliers_iqr, orient=\"index\", columns=[\"n_outliers\"])\n",
        "    .assign(pct_outliers=lambda x: x[\"n_outliers\"] / len(df) * 100)\n",
        "    .sort_values(\"n_outliers\", ascending=False)\n",
        ")\n",
        "\n",
        "outliers_iqr_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPCk5UxPt2D5"
      },
      "source": [
        "Now that we've compared each right-skwed variable with and without logarithms, let's decide which one we'll focus on to eliminate outliers.\n",
        "\n",
        "Based on the results, log transformation proved beneficial for some spending categories:\n",
        "\n",
        "```spend_electronics```: Outliers were reduced from 2.63% to 1.38% (zscore) and 7.01% to 4.07% (IQR) after the log transformation.\n",
        "\n",
        "```spend_meat```: The outlier rate dropped from 0.44% to 0.29% (zscore) and 0.96% to 0.37% (IQR) in its log-transformed state.\n",
        "\n",
        "However, the analysis often revealed an increase in detected anomalies for log-transformed to other variables due to how the transformation compresses the distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcUkTTg3vcJs"
      },
      "outputs": [],
      "source": [
        "numerical.remove(\"spend_groceries_log\")\n",
        "numerical.remove(\"spend_electronics\")\n",
        "numerical.remove(\"spend_vegetables_log\")\n",
        "numerical.remove(\"spend_nonalcohol_log\")\n",
        "numerical.remove(\"spend_alcohol_log\")\n",
        "numerical.remove(\"spend_meat\")\n",
        "numerical.remove(\"spend_fish_log\")\n",
        "numerical.remove(\"spend_hygiene_log\")\n",
        "numerical.remove(\"spend_videogames_log\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psnN_CUC2Afl"
      },
      "outputs": [],
      "source": [
        "df=df.drop(columns = [\"spend_groceries_log\", \"spend_electronics\", \"spend_vegetables_log\", \"spend_nonalcohol_log\", \"spend_alcohol_log\", \"spend_meat\", \"spend_fish_log\", \"spend_hygiene_log\", \"spend_videogames_log\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kChcDxFCKcP4"
      },
      "outputs": [],
      "source": [
        "filters1 = (\n",
        "\n",
        "    (df[\"spend_groceries\"] <= 120000)\n",
        "    &\n",
        "    (df[\"spend_vegetables\"] <= 10000)\n",
        "     &\n",
        "    (df[\"spend_nonalcohol\"] <= 7000)\n",
        "    &\n",
        "    (df[\"spend_alcohol\"] <= 8000)\n",
        "    &\n",
        "    (df[\"spend_fish\"] <= 7000)\n",
        "    &\n",
        "    (df[\"spend_hygiene\"] <= 3000)\n",
        "    &\n",
        "    (df[\"spend_videogames\"] <= 110000)\n",
        "    &\n",
        "    (df[\"spend_petfood\"] <= 2500)\n",
        "     &\n",
        "    (df[\"year_first_transaction\"] > 1995)\n",
        "    &\n",
        "    (df[\"spend_meat_log\"] > 2)\n",
        "    &\n",
        "    (df[\"spend_electronics_log\"] > 2)\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_1 = df[filters1].copy()\n",
        "\n",
        "print(\"Percentage of data removed:\", (len(df) - len(df_1)) / len(df) * 100, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wNcmrb3EhXl"
      },
      "outputs": [],
      "source": [
        "z_scores = np.abs(stats.zscore(df[numerical], nan_policy=\"omit\"))\n",
        "\n",
        "filters2 = (z_scores < 3).all(axis=1)\n",
        "\n",
        "df_2 = df[filters2].copy()\n",
        "\n",
        "print(\"Percentage of data removed:\", (len(df) - len(df_2)) / len(df) * 100, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk2r5e4HCokK"
      },
      "outputs": [],
      "source": [
        "Q1 = df[numerical].quantile(0.25)\n",
        "Q3 = df[numerical].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "upper_lim = Q3 + 1.5 * IQR\n",
        "lower_lim = Q1 - 1.5 * IQR\n",
        "\n",
        "filters3 = ((df[numerical] >= lower_lim) & (df[numerical] <= upper_lim)).all(axis=1)\n",
        "\n",
        "df_3 = df[filters3].copy()\n",
        "\n",
        "print(\"Percentage of data removed:\", (len(df) - len(df_3)) / len(df) * 100, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6L-G1xVCsMr"
      },
      "outputs": [],
      "source": [
        "df_4 = df[ filters1 |filters2 | filters3].copy()\n",
        "\n",
        "print(\"Percentage of data removed:\", (len(df) - len(df_4)) / len(df) * 100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three distinct filtering approaches are evaluated to clean the dataset:\n",
        "\n",
        "Manual Filters (filters1): Custom thresholds are established based on visual inspection and domain logic (e.g., spend_groceries <= 120000), resulting in a 2.76% data removal rate.\n",
        "\n",
        "Z-Score Filter (filters2): A standard $|z| < 3$ threshold is applied, which would remove 19.53% of the data.\n",
        "\n",
        "IQR Filter (filters3): This more aggressive method would result in the removal of 44.38% of the dataset.\n",
        "\n",
        "The final dataset is constructed by applying the operation df_4 = df[filters1 | filters2 | filters3]. Under this logic, an observation is only removed if all three methods classify it as an outlier. If even a single method considers the observation valid, it is retained in the dataset.\n",
        "\n",
        "This approach ensures that only the most extreme and universally agreed-upon anomalies are discarded, resulting in a final removal rate of 2.76% and a clean dataset of 16,365 rows."
      ],
      "metadata": {
        "id": "cFVIcPE-atoI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCpgYt_9EexK"
      },
      "outputs": [],
      "source": [
        "df = df_4.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGi08iysEezm"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAUBT_8RW6ve"
      },
      "source": [
        "# Data Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnHaL87mEe7V"
      },
      "outputs": [],
      "source": [
        "standardScaler = StandardScaler()\n",
        "df[numerical] = standardScaler.fit_transform(df[numerical])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmtnH5hIEe98"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjuDGoaHXHOS"
      },
      "source": [
        "# Data Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the cleaning process, missing values were addressed using a k-Nearest Neighbors (kNN) Imputation strategy with $k=5$. This technique estimates missing entries by identifying the most similar records in the feature space, ensuring a complete and mathematically consistent dataset for subsequent analysis."
      ],
      "metadata": {
        "id": "hjj6RRiVdDiu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97YZrx2hXGbJ"
      },
      "outputs": [],
      "source": [
        "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs3pzZL2XGeR"
      },
      "outputs": [],
      "source": [
        "df[numerical] = imputer.fit_transform(df[numerical])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYemE7i4bWnp"
      },
      "outputs": [],
      "source": [
        "df[numerical] = standardScaler.inverse_transform(df[numerical])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amhq93g0bRd_"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOHjvN57JTmR"
      },
      "outputs": [],
      "source": [
        "ano_atual = datetime.now().year\n",
        "\n",
        "df[\"birthdate\"] = pd.to_datetime(df[\"birthdate\"], errors=\"coerce\")\n",
        "\n",
        "df[\"age\"] = ano_atual - df[\"birthdate\"].dt.year\n",
        "\n",
        "df[\"total_children\"] = df[\"n_kids\"] + df[\"n_teens\"]\n",
        "df[\"has_children\"] = (df[\"total_children\"] > 0).astype(int)\n",
        "\n",
        "def time_period(h):\n",
        "    if 6 <= h < 12:\n",
        "        return \"morning\"\n",
        "    elif 12 <= h < 18:\n",
        "        return \"afternoon\"\n",
        "    else:\n",
        "        return \"evening\"\n",
        "\n",
        "df[\"time_period\"] = df[\"typical_hour\"].apply(time_period)\n",
        "df['loyalty_years'] = ano_atual - df['year_first_transaction']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New variables were derived to capture more nuanced customer behavior:\n",
        "\n",
        "**age**: Calculated by subtracting the birth year from the current year.\n",
        "\n",
        "**total_children & has_children**: Created by summing kids and teens and generating a binary indicator for parenthood.\n",
        "\n",
        "**time_period**: A categorical feature segmenting the typical_hour into morning, afternoon, and evening.\n",
        "\n",
        "**loyalty_years**: Measures the duration of the customer relationship based on the year of the first transaction."
      ],
      "metadata": {
        "id": "2HBjfXnsen_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjOBDBolcGfs"
      },
      "outputs": [],
      "source": [
        "numerical.append(\"age\")\n",
        "numerical.append(\"loyalty_years\")\n",
        "numerical.append(\"total_children\")\n",
        "numerical.append(\"has_children\")\n",
        "categorical.append(\"time_period\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1rqXlOEu8fZ"
      },
      "outputs": [],
      "source": [
        "df['age'] = imputer.fit_transform(df[['age']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEc7VVOSqqXd"
      },
      "outputs": [],
      "source": [
        "df_2=df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HTG-jEjrRAn"
      },
      "source": [
        "# Incoherences II"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwkBkjChraY1"
      },
      "source": [
        "Check if there are customers with a birthdate that would make them minors, but who have a year_first_transaction from 10 years ago"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Egeb5e6p-ch"
      },
      "outputs": [],
      "source": [
        "df_2['age_at_first_purchase'] = df_2['year_first_transaction'] - df_2['birthdate'].dt.year\n",
        "\n",
        "super_childs = df_2[df_2['age_at_first_purchase'] < 10]\n",
        "\n",
        "print(f\"Number of child detected: {len(super_childs)}\")\n",
        "display(super_childs[['name', 'age_at_first_purchase']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if minors have purchased alcohol"
      ],
      "metadata": {
        "id": "ig7cngRQ5d4x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eXi_7kosBIH"
      },
      "outputs": [],
      "source": [
        "underage_alcool = df_2[(df_2['age'] < 18) & (df_2['spend_alcohol'] > 0)]\n",
        "\n",
        "print(f\"Detected {len(underage_alcool)} underage buyers purchasing alcohol.\")\n",
        "display(underage_alcool[['name', 'age', 'spend_alcohol']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jJ-NwV7rP6E"
      },
      "source": [
        "# One-Hot Encoding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m1bdEPPr9R_"
      },
      "outputs": [],
      "source": [
        "def get_ohc_df(df, feats):\n",
        "  ohc = OneHotEncoder(sparse_output=False, drop=\"first\")\n",
        "  ohc_feat = ohc.fit_transform(df[feats])\n",
        "  ohc_feat_names = ohc.get_feature_names_out()\n",
        "  ohc_df = pd.DataFrame(ohc_feat, index=df.index, columns=ohc_feat_names)\n",
        "\n",
        "  df_ohc = pd.concat([df, ohc_df], axis=1)\n",
        "\n",
        "  return df_ohc, ohc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DABXCudKsO6l"
      },
      "outputs": [],
      "source": [
        "df_encoding, ohc = get_ohc_df(df, categorical)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6bHZbhLlV6M"
      },
      "outputs": [],
      "source": [
        "ohc.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One-Hot Encoding (OHE)**: Categorical variables like ```gender``` and ```time_period``` were converted into binary features to be compatible with mathematical models."
      ],
      "metadata": {
        "id": "ZInHEfvKfWuu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfOQBFhRhr0Q"
      },
      "outputs": [],
      "source": [
        "df = df_encoding.drop(columns = [\"id\", \"name\", \"birthdate\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variables such as ```id```, ```name```, and ```birthdate``` were dropped as they do not provide predictive value for modeling."
      ],
      "metadata": {
        "id": "wWGHYTh0fiFP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhbNMaAB7c9x"
      },
      "outputs": [],
      "source": [
        "numerical.append(\"gender_male\")\n",
        "numerical.append(\"time_period_evening\")\n",
        "numerical.append(\"time_period_morning\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I45wISAekRc"
      },
      "source": [
        "# Feature Selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaVJRrYXeuxi"
      },
      "source": [
        "Pearson Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYF8lbAQehn8"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,8))\n",
        "\n",
        "corr = np.round(df[numerical].corr(method=\"pearson\"), decimals=2)\n",
        "\n",
        "mask_annot = np.absolute(corr.values) >= 0.5\n",
        "annot = np.where(mask_annot, corr.values, np.full(corr.shape,\"\"))\n",
        "\n",
        "sns.heatmap(data=corr, annot=annot, cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
        "            fmt='s', vmin=-1, vmax=1, center=0, square=True, linewidths=.5)\n",
        "\n",
        "fig.subplots_adjust(top=0.95)\n",
        "fig.suptitle(\"Correlation Matrix\", fontsize=20)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAmkRM9EJPpM"
      },
      "source": [
        "Dividing features by Perspectives\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lebH_qsnDm05"
      },
      "outputs": [],
      "source": [
        "\n",
        "## Demographic / Behaviour\n",
        "demographic_features = ['n_kids', 'n_teens', 'has_children', 'total_children', 'n_complaints', 'n_stores',\n",
        "    'typical_hour', 'time_period_evening','time_period_morning',\n",
        "    'year_first_transaction', 'loyalty_years', 'latitude', 'longitude',\n",
        "    'has_card','age', 'gender_male',  'pct_promo',\n",
        "    'n_distinct_products'\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "## Preferences\n",
        "preference_features = [\n",
        "  'spend_groceries', 'spend_vegetables', 'spend_nonalcohol',\n",
        "    'spend_alcohol', 'spend_fish', 'spend_hygiene', 'spend_videogames',\n",
        "    'spend_petfood','spend_meat_log', 'spend_electronics_log'\n",
        "]\n",
        "unused_feats= []\n",
        "\n",
        "df_dem = df[demographic_features].copy()\n",
        "df_prf = df[preference_features].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3SyVcM_DnHZ"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,8))\n",
        "\n",
        "corr = np.round(df[demographic_features].corr(method=\"pearson\"), decimals=2)\n",
        "\n",
        "mask_annot = np.absolute(corr.values) >= 0.5\n",
        "annot = np.where(mask_annot, corr.values, np.full(corr.shape,\"\"))\n",
        "\n",
        "sns.heatmap(data=corr, annot=annot, cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
        "            fmt='s', vmin=-1, vmax=1, center=0, square=True, linewidths=.5)\n",
        "\n",
        "fig.subplots_adjust(top=0.95)\n",
        "fig.suptitle(\"Correlation Matrix\", fontsize=20)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGXTJMuilDUD"
      },
      "outputs": [],
      "source": [
        "df_dem = df_dem.drop(columns=[\"n_kids\", \"n_teens\" , \"year_first_transaction\", \"latitude\", \"longitude\", \"time_period_evening\", \"time_period_morning\", \"has_children\", \"has_card\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4Zspgi8-Ra0"
      },
      "outputs": [],
      "source": [
        "demographic_features.remove(\"n_kids\")\n",
        "demographic_features.remove(\"n_teens\")\n",
        "demographic_features.remove(\"year_first_transaction\")\n",
        "demographic_features.remove(\"latitude\")\n",
        "demographic_features.remove(\"longitude\")\n",
        "demographic_features.remove(\"time_period_evening\")\n",
        "demographic_features.remove(\"time_period_morning\")\n",
        "demographic_features.remove(\"has_children\")\n",
        "demographic_features.remove(\"has_card\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLwVZHg-7OdG"
      },
      "outputs": [],
      "source": [
        "numerical.remove(\"n_kids\")\n",
        "numerical.remove(\"n_teens\")\n",
        "numerical.remove(\"year_first_transaction\")\n",
        "numerical.remove(\"latitude\")\n",
        "numerical.remove(\"longitude\")\n",
        "numerical.remove(\"time_period_evening\")\n",
        "numerical.remove(\"time_period_morning\")\n",
        "numerical.remove(\"has_children\")\n",
        "numerical.remove(\"has_card\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Px6F3o2O5Np"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,8))\n",
        "\n",
        "corr = np.round(df[preference_features].corr(method=\"pearson\"), decimals=2)\n",
        "\n",
        "mask_annot = np.absolute(corr.values) >= 0.5\n",
        "annot = np.where(mask_annot, corr.values, np.full(corr.shape,\"\"))\n",
        "\n",
        "sns.heatmap(data=corr, annot=annot, cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
        "            fmt='s', vmin=-1, vmax=1, center=0, square=True, linewidths=.5)\n",
        "\n",
        "fig.subplots_adjust(top=0.95)\n",
        "fig.suptitle(\"Correlation Matrix\", fontsize=20)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm9l4h2XO5W_"
      },
      "outputs": [],
      "source": [
        "df_prf = df_prf.drop(columns=[\"spend_petfood\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_SJQC5W7sSy"
      },
      "outputs": [],
      "source": [
        "numerical.remove(\"spend_petfood\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uupS8TAB-cmd"
      },
      "outputs": [],
      "source": [
        "preference_features.remove(\"spend_petfood\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the cleaning and engineering phases, features are divided into two distinct perspectives, **Demographic / Behaviour** and **Preferences** to facilitate targeted analysis. High correlations and redundancies are identified through correlation matrices, leading to the removal of several variables to ensure model efficiency.\n",
        "\n",
        "After dropping redundant features such as `n_kids`, `latitude`, and `has_card`, the **Demographic / Behaviour** division is composed of `n_complaints`, `n_stores`, `typical_hour`, `loyalty_years`, `age`, `gender_male`, `pct_promo`, and `n_distinct_products`.\n",
        "\n",
        "The **Preferences** division includes consumption metrics such as `spend_groceries`, `spend_vegetables`, `spend_nonalcohol`, `spend_alcohol`, `spend_fish`, `spend_hygiene`, `spend_videogames`, and the log-transformed `spend_meat_log` and `spend_electronics_log`, while `spend_petfood` is excluded."
      ],
      "metadata": {
        "id": "tlLdi8Bwg5hh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMA7oxJASDS8"
      },
      "source": [
        "#Clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nstsb9JuswBm"
      },
      "outputs": [],
      "source": [
        "df1=df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1yiq-tKTFCv"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "df[numerical] = scaler.fit_transform(df[numerical])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIwfupyK8oy3"
      },
      "source": [
        "## Remove noise rows with DBSCAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section describes the application of **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** to identify and remove anomalies that were not captured by previous statistical filters. Unlike standard methods, DBSCAN detects noise based on the spatial density of the data points.\n"
      ],
      "metadata": {
        "id": "EA5V3WgBjCbL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnQzOixWvm96"
      },
      "outputs": [],
      "source": [
        "def split_noise_rows(df, feats, dbs_model):\n",
        "  ## Splits the dataframe into noise and non-noise\n",
        "  dbscan_labels = dbs_model.fit_predict(df[feats])\n",
        "\n",
        "  df_concat = pd.concat([df,\n",
        "                            pd.Series(dbscan_labels,\n",
        "                                      name='dbscan_labels',\n",
        "                                      index=df.index)], axis=1)\n",
        "\n",
        "  df_noise = df_concat[df_concat['dbscan_labels']==-1].copy()\n",
        "  df_nonoise = df_concat[df_concat['dbscan_labels']==0].copy()\n",
        "\n",
        "\n",
        "  return df_noise, df_nonoise, df_concat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wywlAcYteLZp"
      },
      "outputs": [],
      "source": [
        "def plot_kdist_graph(df, feats, n_neighbors=36):\n",
        "  # K-distance graph to find out the right eps value\n",
        "  ## For each data point, we calculate the average distance\n",
        "  ## to its n_neighbors\n",
        "\n",
        "  neigh = NearestNeighbors(n_neighbors=n_neighbors)\n",
        "  neigh.fit(df[feats])\n",
        "  distances, _ = neigh.kneighbors(df[feats])\n",
        "\n",
        "  ## We sort the average distances of the points\n",
        "  ## And plot this\n",
        "  distances = np.sort(distances[:, -1])\n",
        "  plt.ylabel(\"%d-NN Distance\" % n_neighbors)\n",
        "  plt.xlabel(\"Points sorted by distance\")\n",
        "  plt.plot(distances)\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY84iHGuetSJ"
      },
      "outputs": [],
      "source": [
        "numerical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Egp_Qvu5tWsw"
      },
      "outputs": [],
      "source": [
        "plot_kdist_graph(df, numerical)\n",
        "## The \"Knee\" or the threshold before we get very large distances (y-axis)\n",
        "## is at around 3.3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determining Hyperparameters\n",
        "\n",
        "To configure the DBSCAN model, a **k-distance graph** was generated to find the optimal  (epsilon) value.\n",
        "\n",
        "* **Min Samples ():** The `n_neighbors` parameter was set to **36**, which corresponds to the `min_samples` used in the final model (number of variables multipled by 2). This ensures that the distance plotted represents the exact threshold required for a point to be considered a \"core point\" in a cluster.\n",
        "\n",
        "* **Epsilon ():** By analyzing the resulting plot, the \"knee\" or inflection point—where distances begin to increase sharply—was identified at approximately **3.3**. This value represents the maximum radius used to search for neighbors."
      ],
      "metadata": {
        "id": "yhhKLDP6jePx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KepQpdQTvnnI"
      },
      "outputs": [],
      "source": [
        "## Refer to previous notebook for how to tune DBSCAN\n",
        "## Based on the hyperparameters we found\n",
        "\n",
        "dbscan = DBSCAN(eps=3.3, min_samples=36, n_jobs=4)\n",
        "\n",
        "df_noise, df_nonoise, df_combined_noise = split_noise_rows(df, numerical, dbscan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQp3GYAqwSKP"
      },
      "outputs": [],
      "source": [
        "## Rename df_nonoise dataframe to df for simplicity\n",
        "## Remember the df_combined_noise dataframe has both noise and non noise rows\n",
        "\n",
        "print(df_combined_noise.shape)\n",
        "print(df_nonoise.shape)\n",
        "print(df_noise.shape)\n",
        "\n",
        "df = df_nonoise.copy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xvs5wHSfdPV"
      },
      "outputs": [],
      "source": [
        "df_dem = df[demographic_features].copy()\n",
        "df_prf = df[preference_features].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzWFaRnr4Fwa"
      },
      "source": [
        "## Testing different clustering solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjOrMY9B4M7S"
      },
      "outputs": [],
      "source": [
        "# Set up the clusterers\n",
        "kmeans = KMeans(\n",
        "    init='k-means++',\n",
        "    n_init=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "hierarchical = AgglomerativeClustering(\n",
        "    metric='euclidean'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9UhZJAu4C7_"
      },
      "outputs": [],
      "source": [
        "## Some functions we need\n",
        "def get_ss(df):\n",
        "    \"\"\"Computes the sum of squares for all variables given a dataset\n",
        "    \"\"\"\n",
        "    ss = np.sum(df.var() * (df.count() - 1))\n",
        "    return ss  # return sum of sum of squares of each df variable\n",
        "\n",
        "def r2(df, labels):\n",
        "    sst = get_ss(df)\n",
        "    ssw = np.sum(df.groupby(labels).apply(get_ss))\n",
        "    return 1 - ssw/sst\n",
        "\n",
        "def get_r2_scores(df, clusterer, min_k=2, max_k=10):\n",
        "    \"\"\"\n",
        "    Loop over different values of k. To be used with sklearn clusterers.\n",
        "    \"\"\"\n",
        "    r2_clust = {}\n",
        "    for n in range(min_k, max_k):\n",
        "        clust = clone(clusterer).set_params(n_clusters=n)\n",
        "        labels = clust.fit_predict(df)\n",
        "        r2_clust[n] = r2(df, labels)\n",
        "    return r2_clust\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N89o5P6GgWd"
      },
      "outputs": [],
      "source": [
        "def get_r2_df(df, feats, kmeans_model, hierar_model):\n",
        "  # Obtaining the R² scores for each cluster solution\n",
        "\n",
        "  r2_scores = {}\n",
        "  r2_scores['kmeans'] = get_r2_scores(df[feats], kmeans_model)\n",
        "\n",
        "  for linkage in ['complete', 'average', 'single', 'ward']:\n",
        "      r2_scores[linkage] = get_r2_scores(\n",
        "          df[feats], hierar_model.set_params(linkage=linkage)\n",
        "      )\n",
        "\n",
        "  return pd.DataFrame(r2_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct6Jy2CsGi7i"
      },
      "outputs": [],
      "source": [
        "def plot_r2_scores(r2_scores,\n",
        "                   plot_title=\"Demographic Variables:\\nR² plot for various clustering methods\\n\",\n",
        "                   legend_title=\"Cluster methods\"):\n",
        "  # Visualizing the R² scores for each cluster solution on demographic variables\n",
        "  pd.DataFrame(r2_scores).plot.line(figsize=(10,7))\n",
        "\n",
        "  plt.title(plot_title, fontsize=21)\n",
        "  plt.legend(title=legend_title, title_fontsize=11)\n",
        "  plt.xlabel(\"Number of clusters\", fontsize=13)\n",
        "  plt.ylabel(\"R² metric\", fontsize=13)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWH5zGNTGfaM"
      },
      "source": [
        "Finding the optimal clusterer on demographic variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1E3sg17A_jK"
      },
      "outputs": [],
      "source": [
        "## The get_r2_df function takes a few minutes to execute\n",
        "\n",
        "demog_r2_scores = get_r2_df(df, demographic_features, kmeans, hierarchical)\n",
        "demog_r2_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_w-07H85pen"
      },
      "outputs": [],
      "source": [
        "plot_r2_scores(demog_r2_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4Gtn79c5p7J"
      },
      "source": [
        "Finding the optimal clusterer on preferences variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AGc9FPd6YH_"
      },
      "outputs": [],
      "source": [
        "# Set up the clusterers\n",
        "kmeans = KMeans(\n",
        "    init='k-means++',\n",
        "    n_init=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "hierarchical = AgglomerativeClustering(\n",
        "    metric='euclidean'\n",
        ")\n",
        "\n",
        "## The get_r2_df function takes a few minutes to execute\n",
        "\n",
        "pref_r2_scores = get_r2_df(df, preference_features, kmeans, hierarchical)\n",
        "pref_r2_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnU6HkVZ65v2"
      },
      "outputs": [],
      "source": [
        "plot_r2_scores(pref_r2_scores,\n",
        "               plot_title=\"Preference Variables:\\nR² plot for various clustering methods\\n\",\n",
        "               )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the effectiveness of different clustering algorithms, the R^2 metric is utilized to measure the proportion of variance explained by the resulting clusters. Across both feature perspectives, **K-means** and **Ward’s method** consistently demonstrate the highest performance, outperforming other hierarchical techniques such as complete, average, and single linkage.\n",
        "\n",
        "### Demographic Perspectives\n",
        "\n",
        "For the demographic variables, a significant increase in the  value is observed when moving from 2 to **3 clusters**, where the metric reaches approximately **0.23** for K-means and **0.19** for Ward. This number of clusters is selected as it represents an optimal \"elbow\" point, providing a clear segmentation of customer behavior and age groups without over-complicating the model.\n",
        "\n",
        "### Preference Perspectives\n",
        "\n",
        "The preference-based features show even stronger clustering potential, with  values significantly higher than those in the demographic set. Both **K-means** and **Ward** show nearly overlapping performance, with the most substantial gain in variance explained occurring at **4 clusters**. At this level, the models explain roughly **56%** of the total variance, making it the chosen configuration for defining distinct spending profiles.\n"
      ],
      "metadata": {
        "id": "iABvWvtZqMxu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x0EMn2qvBXN"
      },
      "source": [
        "##K-Means\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQvMmzyG0kh"
      },
      "source": [
        "#### Merging Clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-CTPE4FCQ1-"
      },
      "source": [
        "Merge using Hierarchical Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To finalize the segmentation, the cluster results from both the Demographic/Behavior and Preference perspectives were merged into a single, unified classification. This integration allows for a multidimensional view of the customer base, linking \"who the customers are\" with \"how they spend.\""
      ],
      "metadata": {
        "id": "3o3CT74drs9n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxBonj6Au5HQ"
      },
      "outputs": [],
      "source": [
        "perspective_1 = 'pref_labels'\n",
        "perspective_2 = 'demog_labels'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RopKtK4-GvpA"
      },
      "outputs": [],
      "source": [
        "# Applying the right clustering (algorithm and number of clusters) for each perspective\n",
        "\n",
        "kmeans_dem = KMeans(\n",
        "    n_clusters=3,\n",
        "    init='k-means++',\n",
        "    n_init=20,\n",
        "    random_state=42\n",
        ")\n",
        "demog_labels = kmeans_dem.fit_predict(df_dem)\n",
        "\n",
        "df['demog_labels'] = demog_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeP9CEtq8HMi"
      },
      "outputs": [],
      "source": [
        "kmeans_pref = KMeans(\n",
        "    n_clusters=4,\n",
        "    init='k-means++',\n",
        "    n_init=20,\n",
        "    random_state=42\n",
        ")\n",
        "pref_labels = kmeans_pref.fit_predict(df_prf)\n",
        "\n",
        "df['pref_labels'] = pref_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND1BItnoHDFb"
      },
      "outputs": [],
      "source": [
        "# Count label frequencies (contigency table)\n",
        "\n",
        "def make_contingency_table(df, label1, label2):\n",
        "  df_ = df.groupby([label1, label2])\\\n",
        "            .size()\\\n",
        "            .to_frame()\\\n",
        "            .reset_index()\\\n",
        "            .pivot(index=label2, columns=label1)\n",
        "  df_.columns = df_.columns.droplevel()\n",
        "  return df_\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ni2Kzxk68mMI"
      },
      "outputs": [],
      "source": [
        "make_contingency_table(df, perspective_1, perspective_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsBOq480_qzu"
      },
      "outputs": [],
      "source": [
        "def get_mean_bylabel(df, feats, label_name):\n",
        "  # Characterizing the clusters\n",
        "  return df[feats+[label_name]].groupby(label_name).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLajPJnECSy_"
      },
      "outputs": [],
      "source": [
        "# Centroids of the concatenated cluster labels\n",
        "df_hc_centroids = df.groupby([perspective_1, perspective_2])[numerical].mean()\n",
        "df_hc_centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lBL43rdDbuk"
      },
      "outputs": [],
      "source": [
        "## Map combinations of the two labels to their merged cluster label\n",
        "\n",
        "def hc_merge_mapper(df, label1, label2, feats, merged_label, n_clusters=1):\n",
        "  df_ = df.copy()\n",
        "\n",
        "  # Centroids of the concatenated cluster labels\n",
        "  df_centroids = df_.groupby([label1, label2])[feats].mean()\n",
        "\n",
        "  # Re-running the Hierarchical clustering based on the correct number of clusters\n",
        "  hclust = AgglomerativeClustering(\n",
        "      linkage='ward',\n",
        "      metric='euclidean',\n",
        "      n_clusters=n_clusters\n",
        "  )\n",
        "  hclust_labels = hclust.fit_predict(df_centroids)\n",
        "  df_centroids[merged_label] = hclust_labels\n",
        "\n",
        "  cluster_mapper = df_centroids[merged_label].to_dict()\n",
        "\n",
        "  # Mapping the clusters on the centroids to the observations\n",
        "  df_[merged_label] = df_.apply(\n",
        "      lambda row: cluster_mapper[\n",
        "          (row[label1], row[label2])\n",
        "      ], axis=1\n",
        "  )\n",
        "\n",
        "  return df_, df_centroids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d33WR6-AEKxk"
      },
      "outputs": [],
      "source": [
        "df_hc_merged, df_hc_centroids = hc_merge_mapper(df, perspective_1, perspective_2, numerical, 'hc_merged_labels', 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk17pZdaEVvs"
      },
      "outputs": [],
      "source": [
        "df_hc_merged['hc_merged_labels'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Cluster Selection**\n",
        "\n",
        "Result: A final solution of 4 clusters was selected for the merged dataset.\n",
        "\n",
        "Justification: Although a 5-cluster configuration was evaluated, it was ultimately discarded because the fifth segment was found to contain too few observations. Maintaining only 4 clusters ensures that each group is sufficiently large to be statistically significant and actionable for business strategies."
      ],
      "metadata": {
        "id": "NmmD_mMvr-GT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VS2y-DOEpFr"
      },
      "outputs": [],
      "source": [
        "## Get final DF\n",
        "df = df_hc_merged.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3I5SoJZ5F29"
      },
      "source": [
        "### Cluster Profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX1fRGqG2RWZ"
      },
      "outputs": [],
      "source": [
        "def cluster_profiles(df,\n",
        "                     label_columns,\n",
        "                     figsize,\n",
        "                     compar_titles=None,\n",
        "                     colors='Set1'):\n",
        "    \"\"\"\n",
        "    Pass df with labels columns of one or multiple clustering labels.\n",
        "    Then specify this label columns to perform the cluster profile according to them.\n",
        "    \"\"\"\n",
        "    if compar_titles == None:\n",
        "        compar_titles = [\"\"]*len(label_columns)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=len(label_columns), ncols=2,\n",
        "                             figsize=figsize, squeeze=False)\n",
        "    for ax, label, titl in zip(axes, label_columns, compar_titles):\n",
        "\n",
        "        # Filtering df\n",
        "        drop_cols = [i for i in label_columns if i!=label]\n",
        "        dfax = df.drop(drop_cols, axis=1)\n",
        "\n",
        "        # Getting the cluster centroids and counts\n",
        "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
        "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
        "        counts.columns = [label, \"counts\"]\n",
        "\n",
        "        # Setting Data\n",
        "        pd.plotting.parallel_coordinates(centroids, label,\n",
        "                                         color=sns.color_palette(palette=colors), ax=ax[0])\n",
        "        sns.barplot(x=label, y=\"counts\", data=counts, ax=ax[1],\n",
        "                    palette=sns.color_palette(palette=colors))\n",
        "\n",
        "        #Setting Layout\n",
        "        handles, _ = ax[0].get_legend_handles_labels()\n",
        "        cluster_labels = [\"Cluster {}\".format(i) for i in range(len(handles))]\n",
        "        ax[0].annotate(text=titl, xy=(0.95,1.1), xycoords='axes fraction', fontsize=13, fontweight = 'heavy')\n",
        "        ax[0].legend(handles, cluster_labels) # Adaptable to number of clusters\n",
        "        ax[0].axhline(color=\"black\", linestyle=\"--\")\n",
        "        ax[0].set_title(\"Cluster Means - {} Clusters\".format(len(handles)), fontsize=13)\n",
        "        ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=90)\n",
        "        ax[0].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "        ax[1].set_xticklabels(cluster_labels)\n",
        "        ax[1].set_xlabel(\"\")\n",
        "        ax[1].set_ylabel(\"Absolute Frequency\")\n",
        "        ax[1].set_title(\"Cluster Sizes - {} Clusters\".format(len(handles)), fontsize=13)\n",
        "\n",
        "    plt.subplots_adjust(hspace=1.5, top=0.90)\n",
        "    #plt.suptitle(\"Cluster Simple Profilling\", fontsize=23)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJvUy6c2v3Na"
      },
      "outputs": [],
      "source": [
        "# Reminder:\n",
        "# perspective_1 = 'pref_labels'\n",
        "# perspective_2 = 'demog_labels'\n",
        "\n",
        "merged_label_name = 'hc_merged_labels'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBbQ7_v55cNH"
      },
      "outputs": [],
      "source": [
        "labels_list = [perspective_2, perspective_1, merged_label_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zmEWGIj5j8R"
      },
      "outputs": [],
      "source": [
        "# Profilling each cluster (only merged)\n",
        "merged_label_list = [merged_label_name]\n",
        "sns.set(style=\"white\")\n",
        "cluster_profiles(\n",
        "    df = df[numerical + merged_label_list],\n",
        "    label_columns = merged_label_list,\n",
        "    figsize = (27, 8),\n",
        "    compar_titles = [\"Merged clusters profiling\"],\n",
        "    colors='Set2'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKnTIxeP5L2H"
      },
      "outputs": [],
      "source": [
        "# Profilling each cluster (product, behavior, merged)\n",
        "sns.set(style=\"white\")\n",
        "cluster_profiles(\n",
        "    df = df[numerical + labels_list],\n",
        "    label_columns = labels_list,\n",
        "    figsize = (28, 13),\n",
        "    compar_titles = [\"Demographic clustering\", \"Preference clustering\", \"Merged clusters\"],\n",
        "    colors='Set2'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4EgVLz96rP9"
      },
      "source": [
        "### Visualize profiles using heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdNkosJg81ee"
      },
      "outputs": [],
      "source": [
        "def cluster_heatmaps(df,\n",
        "                     label_columns,\n",
        "                     figsize=(20,20),\n",
        "                     compar_titles=None,\n",
        "                     heat_colors='RdYlBu',\n",
        "                     bar_colors='Set2'):\n",
        "    \"\"\"\n",
        "    Pass df with labels columns of one or multiple clustering labels.\n",
        "    Then specify this label columns to perform the cluster profile according to them.\n",
        "    \"\"\"\n",
        "    if compar_titles == None:\n",
        "        compar_titles = [\"\"]*len(label_columns)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=len(label_columns), ncols=2,\n",
        "                             figsize=figsize, squeeze=False)\n",
        "    for ax, label, titl in zip(axes, label_columns, compar_titles):\n",
        "\n",
        "        # Filtering df\n",
        "        drop_cols = [i for i in label_columns if i!=label]\n",
        "        dfax = df.drop(drop_cols, axis=1)\n",
        "\n",
        "        # Getting the cluster centroids and counts\n",
        "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
        "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
        "        counts.columns = [label, \"counts\"]\n",
        "\n",
        "\n",
        "        # Setting Data\n",
        "        handles, _ = ax[0].get_legend_handles_labels()\n",
        "        cluster_labels = [\"Cluster {}\".format(i) for i in range(counts.shape[0])]\n",
        "\n",
        "        sns.heatmap(centroids.drop(columns=label),\n",
        "              square=False, cmap=heat_colors,\n",
        "              ax=ax[0],\n",
        "              )\n",
        "\n",
        "        ax[0].set_title(\"Cluster Means Heatmap - {} Clusters\".format(counts.shape[0]), fontsize=18)\n",
        "        ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=90)\n",
        "        ax[0].set_yticklabels(cluster_labels, rotation=0)\n",
        "        ax[1].annotate(text=titl, xy=(-0.3,1.15),\n",
        "                       xycoords='axes fraction',\n",
        "                       fontsize=18, fontweight = 'heavy')\n",
        "\n",
        "\n",
        "        sns.barplot(y=label, x=\"counts\", data=counts, ax=ax[1], orient='h', palette=bar_colors)\n",
        "        ax[1].set_yticklabels(cluster_labels)\n",
        "        ax[1].set_title(\"Cluster Sizes - {} Clusters\".format(counts.shape[0]), fontsize=18)\n",
        "        ax[1].set_ylabel(\"\")\n",
        "\n",
        "    plt.subplots_adjust(hspace=1, top=0.90)\n",
        "    plt.suptitle(\"Cluster Simple Profilling\", fontsize=23)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEDryKse6yFP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Profilling each cluster (only merged)\n",
        "merged_label_list = [merged_label_name]\n",
        "sns.set(style=\"white\")\n",
        "cluster_heatmaps(\n",
        "    df = df[numerical  + merged_label_list],\n",
        "    label_columns = merged_label_list,\n",
        "    figsize = (27, 8),\n",
        "    compar_titles = [\"Merged clusters profiling\"],\n",
        "    bar_colors='Set2'\n",
        ")\n",
        "\n",
        "sns.set()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADeHskiq6wSq"
      },
      "outputs": [],
      "source": [
        "# Profilling each cluster (product, behavior, merged)\n",
        "sns.set(style=\"whitegrid\")\n",
        "cluster_heatmaps(\n",
        "    df = df[numerical  + labels_list],\n",
        "    label_columns = labels_list,\n",
        "    figsize = (28, 13),\n",
        "    compar_titles = [\"Demographic clustering\", \"Preference clustering\", \"Merged clusters\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZS_Cm5IA4ij"
      },
      "source": [
        "### Tabular cluster characterization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gFAu-EJ-q_M"
      },
      "source": [
        "### Recover original values\n",
        "\n",
        "Remember, we scaled our data during preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at19dO6dtnPQ"
      },
      "outputs": [],
      "source": [
        "## Check the variable name of the scaler you chose to use\n",
        "\n",
        "scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBreKNR7cdmL"
      },
      "outputs": [],
      "source": [
        "scaled_feature_names = scaler.get_feature_names_out()\n",
        "scaled_feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDloDXHo-vBC"
      },
      "outputs": [],
      "source": [
        "## Get the inverse of the transformed values\n",
        "pd.DataFrame(scaler.inverse_transform(df[scaled_feature_names]),\n",
        "             columns=scaled_feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOJtnJI4w54i"
      },
      "outputs": [],
      "source": [
        "## Get the inverse of the transformed values\n",
        "pd.DataFrame(scaler.inverse_transform(df[scaled_feature_names]),\n",
        "             columns=scaled_feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CaJwaU3wzpI"
      },
      "outputs": [],
      "source": [
        "## Put this into a DF\n",
        "\n",
        "df_unscaled = df.copy()\n",
        "\n",
        "df_unscaled[scaled_feature_names] = pd.DataFrame(scaler.inverse_transform(df[scaled_feature_names]),\n",
        "                           index=df.index)\n",
        "\n",
        "df_unscaled.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN0wI9qoAv95"
      },
      "outputs": [],
      "source": [
        "cluster_means = get_mean_bylabel(df_unscaled,\n",
        "                                 numerical,\n",
        "                                 merged_label_name)\n",
        "\n",
        "cluster_means.style.format(precision=2).background_gradient(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38gbY4TfBCO7"
      },
      "outputs": [],
      "source": [
        "## Do the same with pref_labels and demog_labels instead of merged_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcWu0_hVAy5i"
      },
      "outputs": [],
      "source": [
        "cluster_means_1 = get_mean_bylabel(df_unscaled,\n",
        "                                 numerical,\n",
        "                                 perspective_1)\n",
        "cluster_means_1\n",
        "cluster_means_1.style.format(precision=2).background_gradient(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_HMzEcKxYyc"
      },
      "outputs": [],
      "source": [
        "cluster_means_2 = get_mean_bylabel(df_unscaled,\n",
        "                                 numerical,\n",
        "                                 perspective_2)\n",
        "\n",
        "cluster_means_2.round(2)\n",
        "cluster_means_2.style.format(precision=2).background_gradient(axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8siZvsHB2hf"
      },
      "source": [
        "### Non-metric features profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHQ2FyweGknt"
      },
      "outputs": [],
      "source": [
        "print(categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I61by3qZfa35"
      },
      "outputs": [],
      "source": [
        "df[\"time_period\"] = df[\"time_period\"].astype(\"string\")\n",
        "df[\"gender\"] = df[\"gender\"].astype(\"string\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSTH_sAICdmu"
      },
      "outputs": [],
      "source": [
        "## Characteristics considering merged labels segmentation (non metric)\n",
        "## MODE of each feature for each cluster\n",
        "\n",
        "df.groupby([merged_label_name])[categorical].agg(pd.Series.mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bz4sE2yxjZUp"
      },
      "outputs": [],
      "source": [
        "# Defining the variables based on the image\n",
        "non_metric_features = ['gender', 'time_period']\n",
        "merged_label_name = 'hc_merged_labels'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdyiZCkUmNxD"
      },
      "outputs": [],
      "source": [
        "def plot_stacked_bars(df, features, cluster_col):\n",
        "    pastel_colors = ['#FFB7B2', '#FFDAC1', '#E2F0CB', '#B5EAD7', '#C7CEEA', '#F9F7CF']\n",
        "\n",
        "    for feat in features:\n",
        "        cross_tab = pd.crosstab(df[cluster_col], df[feat], normalize='index') * 100\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 6), facecolor='white')\n",
        "\n",
        "\n",
        "        cross_tab.plot(kind='bar', stacked=True, ax=ax, color=pastel_colors, edgecolor='white')\n",
        "\n",
        "\n",
        "        for p in ax.patches:\n",
        "            width, height = p.get_width(), p.get_height()\n",
        "            if height > 5:\n",
        "                ax.annotate(f'{height:.1f}%', (p.get_x() + width/2, p.get_y() + height/2),\n",
        "                            ha='center', va='center', fontsize=9, fontweight='bold', color='#444444')\n",
        "\n",
        "\n",
        "        ax.set_facecolor('white')\n",
        "        ax.grid(False)\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        # ----------------------------------\n",
        "\n",
        "        plt.title(f\"{feat.upper()} DISTRIBUTION\", fontsize=14, pad=20, fontweight='bold')\n",
        "        plt.legend(title=feat, bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)\n",
        "        plt.ylabel(\"Percentage (%)\")\n",
        "        plt.xlabel(\"Cluster Number\")\n",
        "        plt.xticks(rotation=0)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Exemplo de uso:\n",
        "plot_stacked_bars(df, ['gender', 'time_period'], 'hc_merged_labels')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPDqqtV2mNxF"
      },
      "outputs": [],
      "source": [
        "def plot_grouped_bars(df, features, cluster_col):\n",
        "\n",
        "    custom_pastel = ['#FFB7B2', '#FFDAC1', '#E2F0CB', '#B5EAD7', '#C7CEEA', '#F9F7CF']\n",
        "\n",
        "    for feat in features:\n",
        "\n",
        "        sns.set_style(\"white\")\n",
        "        plt.figure(figsize=(12, 6), facecolor='white')\n",
        "\n",
        "\n",
        "        ax = sns.countplot(\n",
        "            data=df,\n",
        "            x=cluster_col,\n",
        "            hue=feat,\n",
        "            palette=custom_pastel\n",
        "        )\n",
        "\n",
        "\n",
        "        for p in ax.patches:\n",
        "            if p.get_height() > 0:\n",
        "                ax.annotate(f'{int(p.get_height())}',\n",
        "                            (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                            ha = 'center', va = 'center',\n",
        "                            xytext = (0, 9),\n",
        "                            textcoords = 'offset points',\n",
        "                            fontsize=10,\n",
        "                            fontweight='bold',\n",
        "                            color='#555555')\n",
        "\n",
        "        plt.title(f\"{feat.upper()} COUNT\", fontsize=15, pad=20, fontweight='bold')\n",
        "        plt.xlabel(\"Cluster Number\", fontsize=12)\n",
        "        plt.ylabel(\"Quantity\", fontsize=12)\n",
        "\n",
        "        plt.legend(title=feat, bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)\n",
        "\n",
        "        sns.despine()\n",
        "\n",
        "\n",
        "        ax.set_facecolor('white')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "plot_grouped_bars(df, ['gender', 'time_period'], 'hc_merged_labels')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iS5foszDuXa"
      },
      "source": [
        "### Cluster Visualization using TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toJotp0ZD09X"
      },
      "outputs": [],
      "source": [
        "def plot_tsne(df, feats, label,\n",
        "              cmap='tab10',\n",
        "              title=\"t-SNE Visualization of K-Means Clustering Solution\"):\n",
        "\n",
        "  two_dim = TSNE(random_state=42).fit_transform(df[feats])\n",
        "  two_dim_df = pd.DataFrame(two_dim, index=df.index)\n",
        "  two_dim_df[label] = df[label]\n",
        "\n",
        "\n",
        "  fig, ax= plt.subplots(figsize=(10,10))\n",
        "  scatter = ax.scatter(x = two_dim_df[0],\n",
        "                      y=two_dim_df[1],\n",
        "                      c=two_dim_df[label],\n",
        "                      s=5,\n",
        "                      cmap=cmap\n",
        "                      )\n",
        "  ax.set_xlabel(\"\")\n",
        "  ax.set_ylabel(\"\")\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "\n",
        "  legend1 = ax.legend(*scatter.legend_elements(),\n",
        "                      loc=\"best\", title=\"Cluster Labels\")\n",
        "  ax.add_artist(legend1)\n",
        "\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oVMQ_27EVcH"
      },
      "outputs": [],
      "source": [
        "plot_tsne(df, numerical, merged_label_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final clustering solution is visualized using t-SNE (t-Distributed Stochastic Neighbor Embedding) to validate the separation of the groups in a two-dimensional space. After merging the results from the Demographic (3 clusters) and Preference (4 clusters) perspectives using Hierarchical Clustering (Ward's Method), a final configuration of 4 clusters is retained.\n",
        "\n"
      ],
      "metadata": {
        "id": "xlTNiY0jtD_n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5asigjZ5UMp_"
      },
      "source": [
        "### Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To interpret the final clustering results and ensure the entire dataset is actionable, a **Decision Tree Classifier** is implemented. This stage serves two primary purposes: providing model interpretability and re-integrating the \"noise\" observations removed during the density-based filtering phase.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Model Training and Performance**\n",
        "\n",
        "A Decision Tree with a `max_depth` of 4 is fitted using the numerical features to predict the merged cluster labels. The model achieves an estimated accuracy of **95.10%** on the test set. This high performance indicates that the clusters are well-defined and clearly separable based on the selected features, confirming the robustness of the previous segmentation steps.\n",
        "\n",
        "### **2. Feature Importance and Interpretability**\n",
        "\n",
        "The importance of each feature in defining the cluster boundaries is assessed to understand the drivers of customer segmentation:\n",
        "\n",
        "* **Primary Drivers:** Spending habits are the most significant predictors, with **`spend_groceries`** (**37.47%**), **`spend_meat_log`** (**36.02%**), and **`spend_videogames`** (**19.33%**) contributing the most to the model's decisions.\n",
        "* **Secondary Drivers:** Features like `spend_electronics_log` and `spend_vegetables` have minor roles, while most demographic variables (e.g., `age`, `gender`, `total_children`) show **zero importance** in this specific classification.\n",
        "This suggests that while demographic data helped group the users initially, their **actual spending behavior** is what truly distinguishes one segment from another.\n",
        "\n",
        "### **3. Re-integrating Noise Observations**\n",
        "\n",
        "The trained Decision Tree is used to predict cluster labels for the **noise rows** that were previously removed by the DBSCAN algorithm. By applying the logic learned from the \"clean\" data, these outliers are assigned to the most statistically similar cluster. This approach allows for the inclusion of the entire population in final business reports or targeted marketing strategies without compromising the initial cluster definitions.\n"
      ],
      "metadata": {
        "id": "sWZD4EPStcbF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--kWVwTkUL_u"
      },
      "outputs": [],
      "source": [
        "# Preparing the data\n",
        "X = df[numerical]\n",
        "y = df[merged_label_name]\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Fitting the decision tree\n",
        "dt = DecisionTreeClassifier(random_state=42, max_depth=4)\n",
        "dt.fit(X_train, y_train)\n",
        "print(\"It is estimated that on average, we are able to predict {0:.2f}% of the customers correctly\".format(dt.score(X_test, y_test)*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viVMhPynAtYL"
      },
      "outputs": [],
      "source": [
        "# Assessing feature importance\n",
        "pd.Series(dt.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_hlfN1-UXQ_"
      },
      "outputs": [],
      "source": [
        "## Remember our noise rows that we removed with DBSCAN?\n",
        "## Predicting the cluster labels of the outliers\n",
        "\n",
        "df_noise[merged_label_name] = dt.predict(df_noise[numerical])\n",
        "df_noise[numerical + [merged_label_name]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHOs4wLZUx-Z"
      },
      "outputs": [],
      "source": [
        "# Visualizing the decision tree\n",
        "num_of_clusters = len(df[merged_label_name].unique())\n",
        "cluster_names = [\"Cluster {}\".format(i) for i in range(num_of_clusters)]\n",
        "dot_data = export_graphviz(dt, out_file=None,\n",
        "                           feature_names=X.columns.to_list(),\n",
        "                           filled=True,\n",
        "                           rounded=True,\n",
        "                           class_names=cluster_names,\n",
        "                           special_characters=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGdL8f8BYqOn"
      },
      "outputs": [],
      "source": [
        "\n",
        "graph = pydotplus.graph_from_dot_data(dot_data)\n",
        "graph.write_png('cluster_decision_tree.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4EmBCAEh0J"
      },
      "source": [
        "## Hierarchical Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlX5ZSqas2eR"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "df1[numerical] = scaler.fit_transform(df1[numerical])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H89MhPopj9mj"
      },
      "source": [
        "### Remove noise rows with DBSCAN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM_njMB9j9mk"
      },
      "outputs": [],
      "source": [
        "def split_noise_rows(df, feats, dbs_model):\n",
        "  ## Splits the dataframe into noise and non-noise\n",
        "  dbscan_labels = dbs_model.fit_predict(df1[feats])\n",
        "\n",
        "  df_concat = pd.concat([df1,\n",
        "                            pd.Series(dbscan_labels,\n",
        "                                      name='dbscan_labels',\n",
        "                                      index=df.index)], axis=1)\n",
        "\n",
        "  df_noise = df_concat[df_concat['dbscan_labels']==-1].copy()\n",
        "  df_nonoise = df_concat[df_concat['dbscan_labels']==0].copy()\n",
        "\n",
        "\n",
        "  return df_noise, df_nonoise, df_concat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zBZd_F4j9ml"
      },
      "outputs": [],
      "source": [
        "## Refer to previous notebook for how to tune DBSCAN\n",
        "## Based on the hyperparameters we found\n",
        "\n",
        "dbscan = DBSCAN(eps=3.3, min_samples=36, n_jobs=4)\n",
        "\n",
        "df_noise, df_nonoise, df_combined_noise = split_noise_rows(df1, numerical, dbscan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy3bK-7Bj9ml"
      },
      "outputs": [],
      "source": [
        "## Rename df_nonoise dataframe to df for simplicity\n",
        "## Remember the df_combined_noise dataframe has both noise and non noise rows\n",
        "\n",
        "print(df_combined_noise.shape)\n",
        "print(df_nonoise.shape)\n",
        "print(df_noise.shape)\n",
        "\n",
        "df1 = df_nonoise.copy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEGwLT9ej9ml"
      },
      "outputs": [],
      "source": [
        "df_dem = df1[demographic_features].copy()\n",
        "df_prf = df1[preference_features].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCvvr6bQ3HxP"
      },
      "outputs": [],
      "source": [
        "perspective_1 = 'pref_labels'\n",
        "perspective_2 = 'demog_labels'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3vDxbBI8oln"
      },
      "outputs": [],
      "source": [
        "# Performing HC\n",
        "hclust_dem = AgglomerativeClustering(linkage='ward',\n",
        "                                 metric='euclidean',\n",
        "                                 n_clusters=3)\n",
        "demog_labels = hclust_dem.fit_predict(df_dem)\n",
        "df1['demog_labels'] = demog_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cHDHnr3FUkR"
      },
      "outputs": [],
      "source": [
        "# Performing HC\n",
        "hclust_pref = AgglomerativeClustering(linkage='ward',\n",
        "                                 metric='euclidean',\n",
        "                                 n_clusters=4)\n",
        "pref_labels = hclust_pref.fit_predict(df_prf)\n",
        "df1['pref_labels'] = pref_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxNgBMeeFUkR"
      },
      "outputs": [],
      "source": [
        "# Count label frequencies (contigency table)\n",
        "\n",
        "def make_contingency_table(df, label1, label2):\n",
        "  df_ = df.groupby([label1, label2])\\\n",
        "            .size()\\\n",
        "            .to_frame()\\\n",
        "            .reset_index()\\\n",
        "            .pivot(index=label2, columns=label1)\n",
        "  df_.columns = df_.columns.droplevel()\n",
        "  return df_\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpOUa-9fFUkR"
      },
      "outputs": [],
      "source": [
        "make_contingency_table(df1, perspective_1, perspective_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J2QJVnrFUkR"
      },
      "outputs": [],
      "source": [
        "def get_mean_bylabel(df1, feats, label_name):\n",
        "  # Characterizing the clusters\n",
        "  return df1[feats+[label_name]].groupby(label_name).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECSmWJBxFUkR"
      },
      "outputs": [],
      "source": [
        "# Centroids of the concatenated cluster labels\n",
        "df_hc_centroids = df1.groupby([perspective_1, perspective_2])[numerical].mean()\n",
        "df_hc_centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzQaxTccFUkS"
      },
      "outputs": [],
      "source": [
        "## Map combinations of the two labels to their merged cluster label\n",
        "\n",
        "def hc_merge_mapper(df, label1, label2, feats, merged_label, n_clusters=1):\n",
        "  df_ = df.copy()\n",
        "\n",
        "  # Centroids of the concatenated cluster labels\n",
        "  df_centroids = df_.groupby([label1, label2])[feats].mean()\n",
        "\n",
        "  # Re-running the Hierarchical clustering based on the correct number of clusters\n",
        "  hclust = AgglomerativeClustering(\n",
        "      linkage='ward',\n",
        "      metric='euclidean',\n",
        "      n_clusters=n_clusters\n",
        "  )\n",
        "  hclust_labels = hclust.fit_predict(df_centroids)\n",
        "  df_centroids[merged_label] = hclust_labels\n",
        "\n",
        "  cluster_mapper = df_centroids[merged_label].to_dict()\n",
        "\n",
        "  # Mapping the clusters on the centroids to the observations\n",
        "  df_[merged_label] = df_.apply(\n",
        "      lambda row: cluster_mapper[\n",
        "          (row[label1], row[label2])\n",
        "      ], axis=1\n",
        "  )\n",
        "\n",
        "  return df_, df_centroids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xAsZ4fwFUkS"
      },
      "outputs": [],
      "source": [
        "df_hc_merged, df_hc_centroids = hc_merge_mapper(df1, perspective_1, perspective_2, numerical, 'hc_merged_labels', 4\n",
        "                                          )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPfVMIIFFUkS"
      },
      "outputs": [],
      "source": [
        "df_hc_merged['hc_merged_labels'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXVpsVTyFUkS"
      },
      "outputs": [],
      "source": [
        "## Get final DF\n",
        "df1 = df_hc_merged.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aREBbICUUM2n"
      },
      "source": [
        "### Cluster Profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivShpR3qUM2q"
      },
      "outputs": [],
      "source": [
        "def cluster_profiles(df1,\n",
        "                     label_columns,\n",
        "                     figsize,\n",
        "                     compar_titles=None,\n",
        "                     colors='Set1'):\n",
        "    \"\"\"\n",
        "    Pass df with labels columns of one or multiple clustering labels.\n",
        "    Then specify this label columns to perform the cluster profile according to them.\n",
        "    \"\"\"\n",
        "    if compar_titles == None:\n",
        "        compar_titles = [\"\"]*len(label_columns)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=len(label_columns), ncols=2,\n",
        "                             figsize=figsize, squeeze=False)\n",
        "    for ax, label, titl in zip(axes, label_columns, compar_titles):\n",
        "\n",
        "        # Filtering df\n",
        "        drop_cols = [i for i in label_columns if i!=label]\n",
        "        dfax = df1.drop(drop_cols, axis=1)\n",
        "\n",
        "        # Getting the cluster centroids and counts\n",
        "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
        "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
        "        counts.columns = [label, \"counts\"]\n",
        "\n",
        "        # Setting Data\n",
        "        pd.plotting.parallel_coordinates(centroids, label,\n",
        "                                         color=sns.color_palette(palette=colors), ax=ax[0])\n",
        "        sns.barplot(x=label, y=\"counts\", data=counts, ax=ax[1],\n",
        "                    palette=sns.color_palette(palette=colors))\n",
        "\n",
        "        #Setting Layout\n",
        "        handles, _ = ax[0].get_legend_handles_labels()\n",
        "        cluster_labels = [\"Cluster {}\".format(i) for i in range(len(handles))]\n",
        "        ax[0].annotate(text=titl, xy=(0.95,1.1), xycoords='axes fraction', fontsize=13, fontweight = 'heavy')\n",
        "        ax[0].legend(handles, cluster_labels) # Adaptable to number of clusters\n",
        "        ax[0].axhline(color=\"black\", linestyle=\"--\")\n",
        "        ax[0].set_title(\"Cluster Means - {} Clusters\".format(len(handles)), fontsize=13)\n",
        "        ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=90)\n",
        "        ax[0].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "        ax[1].set_xticklabels(cluster_labels)\n",
        "        ax[1].set_xlabel(\"\")\n",
        "        ax[1].set_ylabel(\"Absolute Frequency\")\n",
        "        ax[1].set_title(\"Cluster Sizes - {} Clusters\".format(len(handles)), fontsize=13)\n",
        "\n",
        "    plt.subplots_adjust(hspace=2, top=0.90)\n",
        "    plt.suptitle(\"Cluster Simple Profilling\", fontsize=23)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUwgI6iNUM2r"
      },
      "outputs": [],
      "source": [
        "# Reminder:\n",
        "# perspective_1 = 'pref_labels'\n",
        "# perspective_2 = 'demog_labels'\n",
        "\n",
        "merged_label_name = 'hc_merged_labels'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxwtr7NQUM2r"
      },
      "outputs": [],
      "source": [
        "labels_list = [perspective_2, perspective_1, merged_label_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaOtolw_UM2r"
      },
      "outputs": [],
      "source": [
        "# Profilling each cluster (only merged)\n",
        "merged_label_list = [merged_label_name]\n",
        "sns.set(style=\"white\")\n",
        "cluster_profiles(\n",
        "    df1 = df1[numerical + merged_label_list],\n",
        "    label_columns = merged_label_list,\n",
        "    figsize = (27, 8),\n",
        "    compar_titles = [\"Merged clusters profiling\"],\n",
        "    colors='Set2'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQp4aifQUM2s"
      },
      "outputs": [],
      "source": [
        "# Profilling each cluster (product, behavior, merged)\n",
        "sns.set(style=\"white\")\n",
        "cluster_profiles(\n",
        "    df1 = df1[numerical + labels_list],\n",
        "    label_columns = labels_list,\n",
        "    figsize = (28, 13),\n",
        "    compar_titles = [\"Demographic clustering\", \"Preference clustering\", \"Merged clusters\"],\n",
        "    colors='Set2'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_8ehEvtUM2s"
      },
      "source": [
        "### Visualize profiles using heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poaM-cX5UM2s"
      },
      "outputs": [],
      "source": [
        "def cluster_heatmaps(df1,\n",
        "                     label_columns,\n",
        "                     figsize=(20,20),\n",
        "                     compar_titles=None,\n",
        "                     heat_colors='RdYlBu',\n",
        "                     bar_colors='Set2'):\n",
        "    \"\"\"\n",
        "    Pass df with labels columns of one or multiple clustering labels.\n",
        "    Then specify this label columns to perform the cluster profile according to them.\n",
        "    \"\"\"\n",
        "    if compar_titles == None:\n",
        "        compar_titles = [\"\"]*len(label_columns)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=len(label_columns), ncols=2,\n",
        "                             figsize=figsize, squeeze=False)\n",
        "    for ax, label, titl in zip(axes, label_columns, compar_titles):\n",
        "\n",
        "        # Filtering df\n",
        "        drop_cols = [i for i in label_columns if i!=label]\n",
        "        dfax = df1.drop(drop_cols, axis=1)\n",
        "\n",
        "        # Getting the cluster centroids and counts\n",
        "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
        "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
        "        counts.columns = [label, \"counts\"]\n",
        "\n",
        "\n",
        "        # Setting Data\n",
        "        handles, _ = ax[0].get_legend_handles_labels()\n",
        "        cluster_labels = [\"Cluster {}\".format(i) for i in range(counts.shape[0])]\n",
        "\n",
        "        sns.heatmap(centroids.drop(columns=label),\n",
        "              square=False, cmap=heat_colors,\n",
        "              ax=ax[0],\n",
        "              )\n",
        "\n",
        "        ax[0].set_title(\"Cluster Means Heatmap - {} Clusters\".format(counts.shape[0]), fontsize=18)\n",
        "        ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=90)\n",
        "        ax[0].set_yticklabels(cluster_labels, rotation=0)\n",
        "        ax[1].annotate(text=titl, xy=(-0.3,1.15),\n",
        "                       xycoords='axes fraction',\n",
        "                       fontsize=18, fontweight = 'heavy')\n",
        "\n",
        "\n",
        "        sns.barplot(y=label, x=\"counts\", data=counts, ax=ax[1], orient='h', palette=bar_colors)\n",
        "        ax[1].set_yticklabels(cluster_labels)\n",
        "        ax[1].set_title(\"Cluster Sizes - {} Clusters\".format(counts.shape[0]), fontsize=18)\n",
        "        ax[1].set_ylabel(\"\")\n",
        "\n",
        "    plt.subplots_adjust(hspace=2, top=0.90)\n",
        "    plt.suptitle(\"Cluster Simple Profilling\", fontsize=23)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HX24ayFnUM2s"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Profilling each cluster (only merged)\n",
        "merged_label_list = [merged_label_name]\n",
        "sns.set(style=\"white\")\n",
        "cluster_heatmaps(\n",
        "    df1 = df1[numerical  + merged_label_list],\n",
        "    label_columns = merged_label_list,\n",
        "    figsize = (27, 8),\n",
        "    compar_titles = [\"Merged clusters profiling\"],\n",
        "    bar_colors='Set2'\n",
        ")\n",
        "\n",
        "sns.set()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHT0D5o8UM2s"
      },
      "outputs": [],
      "source": [
        "# Profilling each cluster (product, behavior, merged)\n",
        "sns.set(style=\"whitegrid\")\n",
        "cluster_heatmaps(\n",
        "    df1 = df1[numerical  + labels_list],\n",
        "    label_columns = labels_list,\n",
        "    figsize = (28, 13),\n",
        "    compar_titles = [\"Demographic clustering\", \"Preference clustering\", \"Merged clusters\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFn0jPUsUM2s"
      },
      "source": [
        "### Tabular cluster characterization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bF2cieBUM2s"
      },
      "source": [
        "### Recover original values\n",
        "\n",
        "Remember, we scaled our data during preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z--5rhZjUM2t"
      },
      "outputs": [],
      "source": [
        "## Check the variable name of the scaler you chose to use\n",
        "\n",
        "scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP1LA1H1UM2t"
      },
      "outputs": [],
      "source": [
        "scaled_feature_names = scaler.get_feature_names_out()\n",
        "scaled_feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lbz33AKEUM2t"
      },
      "outputs": [],
      "source": [
        "## Get the inverse of the transformed values\n",
        "pd.DataFrame(scaler.inverse_transform(df1[scaled_feature_names]),\n",
        "             columns=scaled_feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCECl5MIUM2t"
      },
      "outputs": [],
      "source": [
        "## Get the inverse of the transformed values\n",
        "pd.DataFrame(scaler.inverse_transform(df1[scaled_feature_names]),\n",
        "             columns=scaled_feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9GbixiNUM2t"
      },
      "outputs": [],
      "source": [
        "## Put this into a DF\n",
        "\n",
        "df_unscaled = df1.copy()\n",
        "\n",
        "df_unscaled[scaled_feature_names] = pd.DataFrame(scaler.inverse_transform(df[scaled_feature_names]),\n",
        "                           index=df.index)\n",
        "\n",
        "df_unscaled.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62WaplAnUM2t"
      },
      "outputs": [],
      "source": [
        "cluster_means = get_mean_bylabel(df_unscaled,\n",
        "                                 numerical,\n",
        "                                 merged_label_name)\n",
        "\n",
        "cluster_means.style.format(precision=2).background_gradient(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY99EJFyUM2t"
      },
      "outputs": [],
      "source": [
        "## Do the same with pref_labels and demog_labels instead of merged_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3G6_kQvUM2t"
      },
      "outputs": [],
      "source": [
        "cluster_means_1 = get_mean_bylabel(df_unscaled,\n",
        "                                 numerical,\n",
        "                                 perspective_1)\n",
        "cluster_means_1\n",
        "cluster_means_1.style.format(precision=2).background_gradient(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgskjLzGUM2u"
      },
      "outputs": [],
      "source": [
        "cluster_means_2 = get_mean_bylabel(df_unscaled,\n",
        "                                 numerical,\n",
        "                                 perspective_2)\n",
        "\n",
        "cluster_means_2.round(2)\n",
        "cluster_means_2.style.format(precision=2).background_gradient(axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdsrHrdtUM2u"
      },
      "source": [
        "### Non-metric features profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3olCiU6xUM2u"
      },
      "outputs": [],
      "source": [
        "print(categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJExpE-1UM2v"
      },
      "outputs": [],
      "source": [
        "## Characteristics considering merged labels segmentation (non metric)\n",
        "## MODE of each feature for each cluster\n",
        "\n",
        "df1.groupby([merged_label_name])[categorical].agg(pd.Series.mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xgi-ta0UM2w"
      },
      "outputs": [],
      "source": [
        "# Defining the variables based on your image\n",
        "non_metric_features = ['gender', 'time_period']\n",
        "merged_label_name = 'hc_merged_labels'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oip-3rifhzgn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_stacked_bars(df, features, cluster_col):\n",
        "    # Defining a custom pastel palette\n",
        "    pastel_colors = ['#FFB7B2', '#FFDAC1', '#E2F0CB', '#B5EAD7', '#C7CEEA', '#F9F7CF']\n",
        "\n",
        "    for feat in features:\n",
        "        cross_tab = pd.crosstab(df[cluster_col], df[feat], normalize='index') * 100\n",
        "\n",
        "        # Creating the figure and defining the white background\n",
        "        fig, ax = plt.subplots(figsize=(10, 6), facecolor='white')\n",
        "\n",
        "        cross_tab.plot(kind='bar', stacked=True, ax=ax, color=pastel_colors, edgecolor='white')\n",
        "\n",
        "        # Add the percentage labels\n",
        "        for p in ax.patches:\n",
        "            width, height = p.get_width(), p.get_height()\n",
        "            if height > 5:\n",
        "                ax.annotate(f'{height:.1f}%', (p.get_x() + width/2, p.get_y() + height/2),\n",
        "                            ha='center', va='center', fontsize=9, fontweight='bold', color='#444444')\n",
        "\n",
        "        # --- BACKGROUND AND AESTHETIC ADJUSTMENTS ---\n",
        "        ax.set_facecolor('white')           # Background of the white chart area\n",
        "        ax.grid(False)                      # remove the grid lines\n",
        "        ax.spines['top'].set_visible(False) # Remove top border\n",
        "        ax.spines['right'].set_visible(False) # Remove right border\n",
        "        # ----------------------------------\n",
        "\n",
        "        plt.title(f\"{feat.upper()} DISTRIBUTION\", fontsize=14, pad=20, fontweight='bold')\n",
        "        plt.legend(title=feat, bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)\n",
        "        plt.ylabel(\"Percentage (%)\")\n",
        "        plt.xlabel(\"Cluster Number\")\n",
        "        plt.xticks(rotation=0)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Exemplo de uso:\n",
        "plot_stacked_bars(df1, ['gender', 'time_period'], 'hc_merged_labels')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6mbnfAijIN5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_grouped_bars(df, features, cluster_col):\n",
        "    # Defining a custom pastel color palette\n",
        "\n",
        "    custom_pastel = ['#FFB7B2', '#FFDAC1', '#E2F0CB', '#B5EAD7', '#C7CEEA', '#F9F7CF']\n",
        "\n",
        "    for feat in features:\n",
        "        # Set the white style before creating the figure\n",
        "        sns.set_style(\"white\")\n",
        "        plt.figure(figsize=(12, 6), facecolor='white')\n",
        "\n",
        "        # Create grouped bar chart\n",
        "        ax = sns.countplot(\n",
        "            data=df,\n",
        "            x=cluster_col,\n",
        "            hue=feat,\n",
        "            palette=custom_pastel\n",
        "        )\n",
        "\n",
        "        # Add value labels on top of the bars\n",
        "        for p in ax.patches:\n",
        "            if p.get_height() > 0:\n",
        "                ax.annotate(f'{int(p.get_height())}',\n",
        "                            (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                            ha = 'center', va = 'center',\n",
        "                            xytext = (0, 9),\n",
        "                            textcoords = 'offset points',\n",
        "                            fontsize=10,\n",
        "                            fontweight='bold',\n",
        "                            color='#555555')\n",
        "\n",
        "        # --- AESTHETIC ADJUSTMENTS ---\n",
        "        plt.title(f\"{feat.upper()} COUNT\", fontsize=15, pad=20, fontweight='bold')\n",
        "        plt.xlabel(\"Cluster Number\", fontsize=12)\n",
        "        plt.ylabel(\"Quantity\", fontsize=12)\n",
        "\n",
        "        # Legend adjustment (white background and no border)\n",
        "        plt.legend(title=feat, bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)\n",
        "\n",
        "        # Remove unnecessary spines (top and right by default)\n",
        "        sns.despine()\n",
        "\n",
        "        # Ensure the axis background is also white\n",
        "        ax.set_facecolor('white')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Function call\n",
        "plot_grouped_bars(df1, ['gender', 'time_period'], 'hc_merged_labels')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jruGNKcGa2OG"
      },
      "source": [
        "### Cluster Visualization using TSNE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_tsne(df, feats, label,\n",
        "              cmap='tab10',\n",
        "              title=\"t-SNE Visualization of Ward Clustering Solution\"):\n",
        "\n",
        "  two_dim = TSNE(random_state=42).fit_transform(df[feats])\n",
        "  two_dim_df = pd.DataFrame(two_dim, index=df.index)\n",
        "  two_dim_df[label] = df[label]\n",
        "\n",
        "\n",
        "  fig, ax= plt.subplots(figsize=(10,10))\n",
        "  scatter = ax.scatter(x = two_dim_df[0],\n",
        "                      y=two_dim_df[1],\n",
        "                      c=two_dim_df[label],\n",
        "                      s=5,\n",
        "                      cmap=cmap\n",
        "                      )\n",
        "  ax.set_xlabel(\"\")\n",
        "  ax.set_ylabel(\"\")\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "\n",
        "  legend1 = ax.legend(*scatter.legend_elements(),\n",
        "                      loc=\"best\", title=\"Cluster Labels\")\n",
        "  ax.add_artist(legend1)\n",
        "\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "CeCckRECPdW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz_3QcS4HEdz"
      },
      "outputs": [],
      "source": [
        "plot_tsne(df1, numerical, merged_label_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Comparative Analysis: K-Means vs. Ward’s Method\n",
        "\n",
        "The consistency of the identified segments is further validated by comparing **K-Means** with **Ward’s Hierarchical Method**. While both algorithms identify virtually identical general groups, the **t-SNE** visualization reveals that **Ward’s Method** produces significantly more **compact and well-separated clusters**. Ward’s algorithm better handles the nuances of the merged perspectives, minimizing overlap and highlighting the distinct boundaries of the four final segments.\n"
      ],
      "metadata": {
        "id": "buAsPyLjvYN-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7agxPvWpa2OI"
      },
      "source": [
        "### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adeRJK1xa2OI"
      },
      "outputs": [],
      "source": [
        "# Preparing the data\n",
        "X = df1[numerical]\n",
        "y = df1[merged_label_name]\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Fitting the decision tree\n",
        "dt = DecisionTreeClassifier(random_state=42, max_depth=4)\n",
        "dt.fit(X_train, y_train)\n",
        "print(\"It is estimated that on average, we are able to predict {0:.2f}% of the customers correctly\".format(dt.score(X_test, y_test)*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW2F_JmIa2OI"
      },
      "outputs": [],
      "source": [
        "# Assessing feature importance\n",
        "pd.Series(dt.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRQuV4lma2OI"
      },
      "outputs": [],
      "source": [
        "## Predicting the cluster labels of the outliers\n",
        "\n",
        "df_noise[merged_label_name] = dt.predict(df_noise[numerical])\n",
        "df_noise[numerical + [merged_label_name]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHiJO0Lqa2OI"
      },
      "outputs": [],
      "source": [
        "# Visualizing the decision tree\n",
        "num_of_clusters = len(df1[merged_label_name].unique())\n",
        "cluster_names = [\"Cluster {}\".format(i) for i in range(num_of_clusters)]\n",
        "dot_data = export_graphviz(dt, out_file=None,\n",
        "                           feature_names=X.columns.to_list(),\n",
        "                           filled=True,\n",
        "                           rounded=True,\n",
        "                           class_names=cluster_names,\n",
        "                           special_characters=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwer8nzma2OI"
      },
      "outputs": [],
      "source": [
        "### optionally save dt png\n",
        "graph = pydotplus.graph_from_dot_data(dot_data)\n",
        "graph.write_png('cluster_decision_tree2.png')\n",
        "\n",
        "## Open sidebar to download the image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Profilling"
      ],
      "metadata": {
        "id": "nbtEuE83vuZl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpNSt-tVxdP5"
      },
      "outputs": [],
      "source": [
        "# Profilling each cluster (only merged)\n",
        "merged_label_list = [merged_label_name]\n",
        "sns.set(style=\"white\")\n",
        "cluster_profiles(\n",
        "    df1 = df1[numerical + merged_label_list],\n",
        "    label_columns = merged_label_list,\n",
        "    figsize = (27, 8),\n",
        "    compar_titles = [\"Merged clusters profiling\"],\n",
        "    colors='Set2'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g6Uidj4xdP6"
      },
      "outputs": [],
      "source": [
        "cluster_means = get_mean_bylabel(df_unscaled,\n",
        "                                 numerical,\n",
        "                                 merged_label_name)\n",
        "\n",
        "cluster_means.style.format(precision=2).background_gradient(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Final Cluster Profiling: Merged Perspectives**\n",
        "\n",
        "Based on the integration of demographic and preference data, the following four segments have been identified as the definitive customer profiles:\n",
        "\n",
        "#### **Cluster 0: The Price-Sensitive & Fussy Shoppers**\n",
        "\n",
        "* **Behavioural Profile**: This group is behaviourally distinct due to high interaction levels; they have the highest average **complaints** (**1.29**) and visit the most physical **stores** (**2.81**). They are \"Cherry Pickers\" who rely heavily on **promotions** (**0.64** average `pct_promo`).\n",
        "* **Spending Habits**: Their budget is strictly managed, showing the lowest expenditure on **groceries** (**9,683.17**) and low interest in high-ticket categories.\n",
        "* **Demographic Context**: Average age of **55.7** with a moderate number of children (**1.51**).\n",
        "* **Strategic Insight**: To capture value here, the business should use deep-discount loss-leaders to drive foot traffic, as these shoppers are highly mobile and willing to switch stores for a better price.\n",
        "\n",
        "#### **Cluster 1: The Niche Entertainment Enthusiasts**\n",
        "\n",
        "* **Behavioural Profile**: This group is defined by extreme spending in leisure categories. They lead the dataset in **Video Games** (**5,005.34**) and **Alcohol** (**2,565.19**).\n",
        "* **Spending Habits**: They show significantly lower interest in household staples like fresh produce, focusing their disposable income on entertainment and electronics.\n",
        "* **Demographic Context**: They represent the \"Younger\" profile with the fewest children (**0.40**) and a preference for late-night shopping (**typical_hour: 17.81**).\n",
        "* **Strategic Insight**: This is a \"leisure\" shopper. Marketing should focus on high-end tech launches, gaming accessories, and premium beverage selections.\n",
        "\n",
        "#### **Cluster 2: The Premium Family Pillar**\n",
        "\n",
        "* **Size**: ~6,800 observations (the largest and most profitable segment).\n",
        "* **Behavioural Profile**: This cluster is the \"engine\" of the store, dominating spending across all core fresh categories: **Groceries** (**45,007.24**), **Fish** (**2,785.22**), and **Meat** (log: **8.23**).\n",
        "* **Demographic Context**: They perfectly align with a \"Traditional Family\" profile, having the highest child count (**4.35**) and high overall loyalty.\n",
        "* **Strategic Insight**: Loyalty is key here. Strategies should include family-sized packaging, high-quality fresh product guarantees, and multi-buy offers on household staples.\n",
        "\n",
        "#### **Cluster 3: The Health-Conscious Minimalists**\n",
        "\n",
        "* **Behavioural Profile**: Defined by a disciplined and specific diet. They show the highest focus on **Vegetables** (**4,924.48**) while significantly reducing expenditure on **Meat** (log: **5.96**).\n",
        "* **Spending Habits**: Their basket is \"clean\" and focused on natural, fresh produce rather than processed goods or non-essentials.\n",
        "* **Demographic Context**: Stable, middle-aged shoppers (avg age **55.9**) with a moderate family size (**2.39** children).\n",
        "* **Strategic Insight**: This group is motivated by health. Cross-selling organic products, plant-based meat alternatives, and eco-friendly household goods would be highly effective.\n",
        "\n"
      ],
      "metadata": {
        "id": "gWhnDISKykld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjErtJgXy8uv"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_stacked_bars(df, features, cluster_col):\n",
        "    # Defining a custom pastel palette\n",
        "    pastel_colors = ['#FFB7B2', '#FFDAC1', '#E2F0CB', '#B5EAD7', '#C7CEEA', '#F9F7CF']\n",
        "\n",
        "    for feat in features:\n",
        "        cross_tab = pd.crosstab(df[cluster_col], df[feat], normalize='index') * 100\n",
        "\n",
        "        # Creating the figure and defining the white background\n",
        "        fig, ax = plt.subplots(figsize=(10, 6), facecolor='white')\n",
        "\n",
        "        cross_tab.plot(kind='bar', stacked=True, ax=ax, color=pastel_colors, edgecolor='white')\n",
        "\n",
        "        # Add the percentage labels\n",
        "        for p in ax.patches:\n",
        "            width, height = p.get_width(), p.get_height()\n",
        "            if height > 5:\n",
        "                ax.annotate(f'{height:.1f}%', (p.get_x() + width/2, p.get_y() + height/2),\n",
        "                            ha='center', va='center', fontsize=9, fontweight='bold', color='#444444')\n",
        "\n",
        "        # --- BACKGROUND AND AESTHETIC ADJUSTMENTS ---\n",
        "        ax.set_facecolor('white')           # Background of the white chart area\n",
        "        ax.grid(False)                      # remove the grid lines\n",
        "        ax.spines['top'].set_visible(False) # Remove top border\n",
        "        ax.spines['right'].set_visible(False) # Remove right border\n",
        "        # ----------------------------------\n",
        "\n",
        "        plt.title(f\"{feat.upper()} DISTRIBUTION\", fontsize=14, pad=20, fontweight='bold')\n",
        "        plt.legend(title=feat, bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)\n",
        "        plt.ylabel(\"Percentage (%)\")\n",
        "        plt.xlabel(\"Cluster Number\")\n",
        "        plt.xticks(rotation=0)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Exemplo de uso:\n",
        "plot_stacked_bars(df1, ['gender', 'time_period'], 'hc_merged_labels')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The integration of temporal data and gender demographics provides some context for the final segments:\n",
        "\n",
        "**Temporal Patterns**: Shopping schedules vary significantly by lifestyle. Cluster 1 (Gamers) is the most nocturnal group, conducting 60.6% of their purchases in the evening. Cluster 2 (Families) maintains a more traditional routine with the highest morning activity at 38.7%. Clusters 0 (Deal hunters) and 3 (Vegans) predominantly shop during the afternoon.\n",
        "\n",
        "**Gender Distribution:** The data reveals a near-perfect balance across all segments, with a consistent 50/50 split between male and female customers. This indicates that consumption drivers, such as the high-tech focus in Cluster 1 or the health focus in Cluster 3, are independent of gender."
      ],
      "metadata": {
        "id": "cwigDxmLy-Qu"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}